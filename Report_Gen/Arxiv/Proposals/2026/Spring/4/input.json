{
  "Version": "4",
  "Year": "2026",
  "Semester": "Spring",
  "project_name": "Deep Learning for Video Understanding: Action Recognition and Temporal Feature Extraction",
  "Objective": "\n            The goal of this project is to develop a comprehensive video understanding system that can classify \n            human actions, extract temporal features, and perform exploratory data analysis on video datasets. \n            Students will explore state-of-the-art deep learning architectures for video processing and build \n            end-to-end pipelines from raw video to actionable insights.\n\n            Key Objectives:\n            1. Build a robust video preprocessing pipeline that can:\n               - Extract frames from videos at various frame rates (1 fps, 5 fps, 30 fps)\n               - Perform video quality assessment (resolution, bitrate, compression artifacts)\n               - Extract optical flow and motion features\n               - Handle various video formats (MP4, AVI, MKV, WebM)\n               - Generate video thumbnails and key frames\n\n            2. Develop comprehensive video EDA (Exploratory Data Analysis) toolkit:\n               - Statistical analysis: video length distribution, frame count, fps analysis\n               - Visual analysis: frame diversity, motion intensity, scene complexity\n               - Temporal analysis: shot detection, scene transitions, temporal patterns\n               - Content analysis: object presence, dominant colors, brightness levels\n               - Annotation analysis: class distribution, temporal annotations\n\n            3. Implement and compare multiple video classification architectures:\n               - 2D CNNs with temporal aggregation (ResNet-50 + LSTM)\n               - 3D CNNs for spatiotemporal learning (C3D, I3D, R(2+1)D)\n               - Two-stream networks (spatial + temporal streams)\n               - Vision transformers for video (ViViT, TimeSformer, Video Swin Transformer)\n               - Efficient models for deployment (MobileNetV3 + GRU, X3D)\n\n            4. Extract and visualize temporal features:\n               - Frame-level embeddings from pre-trained models\n               - Temporal attention weights showing important frames\n               - Action localization in untrimmed videos\n               - Video summarization using key frame extraction\n\n            5. Build practical applications:\n               - Real-time action recognition from webcam\n               - Video search and retrieval system\n               - Automated video highlights generation\n               - Anomaly detection in surveillance videos\n\n            6. Create interactive visualization dashboard:\n               - Video playback with frame-by-frame analysis\n               - Feature embedding visualization (t-SNE, UMAP)\n               - Confusion matrices and per-class metrics\n               - Temporal heatmaps showing model attention\n\n            7. Deploy as accessible tool:\n               - Web application for video upload and analysis\n               - REST API for batch video processing\n               - Command-line tool for researchers\n               - Documentation and tutorials\n            ",
  "Dataset": "\n            All datasets are publicly available with no access restrictions:\n\n            PRIMARY DATASETS (Action Recognition):\n\n            1. UCF101 (RECOMMENDED FOR BEGINNERS):\n               - URL: https://www.crcv.ucf.edu/data/UCF101.php\n               - Alternative: https://www.kaggle.com/datasets/matthewjansen/ucf101-action-recognition\n               - Size: 13,320 videos, ~6.5 GB\n               - Classes: 101 human action categories\n               - Content: YouTube videos of actions (sports, instruments, human-object interactions)\n               - Duration: 2-16 seconds per video\n               - Resolution: 320\u00d7240 pixels\n               - Splits: 3 official train/test splits provided\n               - Format: AVI files\n               - Download: Direct download or Kaggle\n               - Time: 15-20 minutes\n               - Paper: https://arxiv.org/abs/1212.0402\n\n            2. HMDB51 (Human Motion Database):\n               - URL: https://serre-lab.clps.brown.edu/resource/hmdb-a-large-human-motion-database/\n               - Size: 7,000 videos, ~2 GB\n               - Classes: 51 action categories\n               - Content: Movies and YouTube clips\n               - Duration: Variable length\n               - Splits: 3 official train/test splits\n               - Format: AVI files\n               - Download: Direct download (registration required but instant)\n               - Time: 5-10 minutes\n               - Paper: https://ieeexplore.ieee.org/document/6126543\n\n            3. Kinetics-400 (Large-Scale):\n               - URL: https://github.com/cvdfoundation/kinetics-dataset\n               - Alternative: https://www.kaggle.com/datasets/shivamb/kinetics-400-mini (preprocessed mini)\n               - Size: ~400,000 videos, ~450 GB (full) or ~5 GB (mini)\n               - Classes: 400 human action classes\n               - Content: YouTube clips\n               - Duration: ~10 seconds per clip\n               - Resolution: Variable (typically 360p or higher)\n               - Format: MP4 files\n               - Download: Use provided downloader script or Kaggle preprocessed version\n               - Time: Several hours for full dataset (recommend using preprocessed subset)\n               - Paper: https://arxiv.org/abs/1705.06950\n               - NOTE: Use Kinetics-400-Mini (5000 videos, 5 GB) for faster prototyping\n\n            4. Something-Something V2 (Fine-Grained Actions):\n               - URL: https://developer.qualcomm.com/software/ai-datasets/something-something\n               - Size: 220,847 videos, ~20 GB\n               - Classes: 174 action classes (e.g., \"putting X into Y\", \"taking X from Y\")\n               - Content: Crowdsourced videos of humans interacting with objects\n               - Duration: 2-6 seconds\n               - Format: WebM files\n               - Download: Free registration required (instant approval)\n               - Time: 30-45 minutes\n               - Paper: https://arxiv.org/abs/1706.04261\n\n            5. Moments in Time (Scene Understanding):\n               - URL: http://moments.csail.mit.edu/\n               - Size: 1 million videos, ~100 GB (full) or ~5 GB (mini subset)\n               - Classes: 339 different actions and activities\n               - Content: 3-second clips from various sources\n               - Duration: 3 seconds per video\n               - Format: MP4 files\n               - Download: Direct download or via provided scripts\n               - Time: 10-15 minutes for mini, several hours for full\n               - Paper: https://arxiv.org/abs/1801.03150\n\n\n            SPECIALIZED DATASETS:\n\n            6. ActivityNet (Untrimmed Videos - Advanced):\n               - URL: http://activity-net.org/download.html\n               - Size: 20,000 untrimmed videos, ~500 GB\n               - Classes: 200 activity classes\n               - Content: Long YouTube videos with temporal annotations\n               - Duration: 1-10 minutes per video\n               - Format: MP4 files\n               - Download: YouTube downloader provided\n               - Time: Several hours\n               - Use Case: Temporal action localization\n               - Paper: https://arxiv.org/abs/1705.00754\n\n            7. UCF Crime Dataset (Anomaly Detection):\n               - URL: https://www.crcv.ucf.edu/projects/real-world/\n               - Size: 1,900 videos, ~30 GB\n               - Classes: 13 anomaly types (robbery, assault, etc.)\n               - Content: Surveillance videos\n               - Duration: Variable length (untrimmed)\n               - Format: MP4 files\n               - Download: Direct download\n               - Time: 45-60 minutes\n               - Use Case: Video anomaly detection\n\n            8. MSR-VTT (Video Captioning):\n               - URL: https://www.microsoft.com/en-us/research/publication/msr-vtt-a-large-video-description-dataset-for-bridging-video-and-language/\n               - Alternative: https://www.kaggle.com/datasets/vishnutheepb/msr-vtt\n               - Size: 10,000 videos, ~40 GB\n               - Content: Video clips with text descriptions\n               - Duration: 10-30 seconds\n               - Format: MP4 + JSON annotations\n               - Download: Kaggle or official site\n               - Time: 60-90 minutes\n               - Use Case: Video-text multimodal learning\n\n\n            PRE-EXTRACTED FEATURES (For Faster Prototyping):\n\n            9. UCF101 Features:\n               - I3D Features: https://github.com/hassony2/kinetics_i3d_pytorch\n               - R(2+1)D Features: Available on Kaggle\n               - Size: ~500 MB (much smaller than raw videos)\n               - Format: NumPy arrays (.npy files)\n               - Use Case: Skip video processing, directly train classifiers\n\n\n            DATASET SELECTION GUIDE:\n\n            Beginner (Start Here):\n            - UCF101 (6.5 GB, 13K videos) - Best for learning\n            - HMDB51 (2 GB, 7K videos) - Good for quick experiments\n\n            Intermediate:\n            - Kinetics-400-Mini (5 GB, 5K videos) - Subset for prototyping\n            - Something-Something V2 (20 GB, 220K videos) - Fine-grained actions\n\n            Advanced:\n            - Kinetics-400 Full (450 GB) - Large-scale training\n            - ActivityNet (500 GB) - Temporal localization\n\n\n            RECOMMENDED STARTING POINT:\n            Use UCF101 for initial development (small, well-documented, fast to download), then scale up \n            to Kinetics-400 or Something-Something V2 for final experiments and paper results.\n            ",
  "Rationale": "\n            Video understanding is a fundamental challenge in computer vision with applications spanning \n            surveillance, autonomous vehicles, healthcare, sports analytics, entertainment, and human-computer \n            interaction. Unlike images, videos contain rich temporal information that requires specialized \n            architectures to capture motion patterns and temporal dependencies.\n\n            WHY THIS PROJECT IS TIMELY AND HIGHLY PUBLISHABLE:\n\n            1. TRANSFORMERS FOR VIDEO ARE TRENDING:\n               - Vision transformers (ViT) recently dominated image classification\n               - Video transformers (ViViT, TimeSformer, Video Swin) are now state-of-the-art\n               - Opportunity to compare against 3D CNNs and show efficiency gains\n               - Self-attention reveals interpretable temporal patterns\n\n            2. EFFICIENT VIDEO MODELS ARE CRITICAL:\n               - Real-time video processing requires lightweight architectures\n               - Mobile deployment is increasingly important (edge computing)\n               - Model compression and knowledge distillation for video is under-explored\n               - X3D, MobileViT-V2 show promise but need more evaluation\n\n            3. TEMPORAL FEATURE EXTRACTION IS VALUABLE:\n               - Pre-trained video models (like CLIP for images) are emerging\n               - Transfer learning for video is less mature than for images\n               - Feature visualization helps understand what models learn\n               - Applications in video retrieval, summarization, captioning\n\n            4. MULTIMODAL VIDEO UNDERSTANDING:\n               - Combining visual, audio, and text (video captions) is hot\n               - Few works systematically study multimodal fusion for video\n               - CLIP4Clip, Frozen, etc. show potential but need more research\n\n            5. PRACTICAL APPLICATIONS:\n               - Sports analytics: automated highlight detection, player tracking\n               - Healthcare: fall detection, patient monitoring, surgical video analysis\n               - Security: anomaly detection, crowd analysis, violence detection\n               - Education: automated lecture summarization, student engagement detection\n               - Entertainment: video recommendation, content moderation, trend detection\n\n            6. INTERPRETABILITY IMPERATIVE:\n               - Black-box video models are hard to trust for critical applications\n               - Attention visualization shows which frames matter\n               - Saliency maps highlight important spatial regions over time\n               - Temporal class activation maps (TCAM) for action localization\n\n            7. PUBLICATION VENUES (STRONG ACCEPTANCE FOR QUALITY WORK):\n\n               Top Computer Vision Conferences:\n               - CVPR (Computer Vision and Pattern Recognition) - flagship CV conference\n               - ICCV (International Conference on Computer Vision)\n               - ECCV (European Conference on Computer Vision)\n               - BMVC (British Machine Vision Conference)\n\n               Machine Learning Conferences:\n               - NeurIPS (Neural Information Processing Systems) - video learning track\n               - ICML (International Conference on Machine Learning)\n               - ICLR (International Conference on Learning Representations)\n\n               Multimedia & Video:\n               - ACM Multimedia (MM) - dedicated video understanding track\n               - ICME (International Conference on Multimedia and Expo)\n               - MMM (MultiMedia Modeling)\n\n               AI Conferences:\n               - AAAI (Association for the Advancement of AI)\n               - IJCAI (International Joint Conference on AI)\n\n               Specialized Workshops:\n               - CVPR Workshop on Large Scale Video Understanding\n               - ICCV Workshop on Video Analysis and Understanding\n               - NeurIPS Workshop on Self-Supervised Learning\n\n               Journals:\n               - IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)\n               - International Journal of Computer Vision (IJCV)\n               - IEEE Transactions on Multimedia (TMM)\n               - Computer Vision and Image Understanding (CVIU)\n\n\n            NOVELTY AND CONTRIBUTION OPPORTUNITIES:\n\n            Students can contribute by:\n            - Comprehensive benchmark comparing 2D CNNs, 3D CNNs, and transformers on same datasets\n            - Efficient video transformers with reduced computational cost\n            - Novel temporal attention mechanisms (hierarchical, sparse, adaptive)\n            - Transfer learning study: ImageNet \u2192 UCF101 \u2192 Kinetics \u2192 target domain\n            - Few-shot video classification with meta-learning\n            - Self-supervised pre-training for video (contrastive learning, masked autoencoders)\n            - Temporal action detection in untrimmed videos\n            - Video quality assessment using deep learning\n            - Explainable video classification with attention visualization\n            - Real-time video understanding on edge devices (Raspberry Pi, mobile)\n\n\n            RESEARCH QUESTIONS TO EXPLORE:\n\n            1. How do different temporal modeling approaches compare? (LSTM vs. 3D Conv vs. Transformer)\n            2. What is the optimal frame sampling rate for different action types?\n            3. Do pre-trained image models transfer well to video tasks?\n            4. How much does optical flow improve action recognition?\n            5. Can we detect actions with minimal frames (1-5 frames)?\n            6. What temporal patterns do models learn? (visualized via attention)\n            7. How robust are video models to video compression and quality degradation?\n            8. Can we achieve real-time performance on consumer hardware?\n\n\n            BROADER IMPACT:\n\n            - Accessibility: Tools for automatically captioning videos for hearing-impaired users\n            - Safety: Automated detection of dangerous situations in surveillance\n            - Healthcare: Non-intrusive monitoring of elderly patients\n            - Education: Analyzing student engagement in online learning\n            - Sports: Democratizing advanced analytics for amateur athletes\n            - Environment: Wildlife monitoring and behavior analysis\n            ",
  "Approach": "\n            (Due to character limits, this will be provided in the complete Python file)\n            See complete implementation details in the full proposal document.\n\n            PHASE 1: VIDEO DATA PIPELINE & EDA (Weeks 1-2)\n            PHASE 2: BASELINE MODELS - 2D CNN + TEMPORAL AGGREGATION (Weeks 3-4)\n            PHASE 3: 3D CONVOLUTIONAL NETWORKS (Weeks 5-6)\n            PHASE 4: VIDEO TRANSFORMERS (Weeks 7-8)\n            PHASE 5: TEMPORAL FEATURE EXTRACTION & VISUALIZATION (Weeks 9-10)\n            PHASE 6: DEPLOYMENT & APPLICATIONS (Weeks 11-12)\n            PHASE 7: PAPER WRITING & WEB APPLICATION (Weeks 13-14)\n            ",
  "Timeline": "\n            Week 1:     Dataset Download & Video EDA\n            Week 2:     Advanced Feature Extraction (Frames, Optical Flow, Motion Analysis)\n            Week 3:     2D CNN + LSTM Baseline\n            Week 4:     Alternative Temporal Aggregation (AvgPool, TempConv, GRU)\n            Week 5:     3D CNN Implementation (C3D, I3D)\n            Week 6:     Two-Stream Networks (RGB + Optical Flow)\n            Week 7:     Video Transformers (ViViT)\n            Week 8:     TimeSformer & Divided Attention\n            Week 9:     Feature Extraction & Embedding Visualization\n            Week 10:    Temporal Attention Visualization & Interpretability\n            Week 11:    Real-Time Classification & Webcam Demo\n            Week 12:    Video Highlights & Practical Applications\n            Week 13:    Research Paper Writing\n            Week 14:    Web Application Development & Code Release\n\n            TOTAL: 14 weeks (one semester)\n\n            KEY MILESTONES:\n            - Week 2:  Clean dataset, EDA complete, frame extraction pipeline working\n            - Week 4:  Baseline 2D models trained with results\n            - Week 6:  3D CNN and two-stream models implemented\n            - Week 8:  Video transformers trained\n            - Week 10: All experiments complete, visualizations ready\n            - Week 12: Applications built and tested\n            - Week 14: Paper submitted, code published, demo deployed\n\n            DELIVERABLES BY WEEK 14:\n            - 8-page conference paper (CVPR format)\n            - GitHub repository with full implementation\n            - Pre-trained models on Hugging Face/Google Drive\n            - Interactive web demo (Streamlit/Gradio)\n            - Video demonstrations of applications\n            - Comprehensive documentation and tutorials\n            - Blog post explaining methodology\n            - Presentation slides and poster\n            ",
  "Expected Number Students": "\n            RECOMMENDED: 2-3 students\n\n            ROLE DISTRIBUTION FOR 3 STUDENTS:\n\n            Student 1: Data Engineer & EDA Specialist\n            - Responsibilities:\n              * Download and organize video datasets\n              * Build video preprocessing pipeline (frame extraction, optical flow)\n              * Perform comprehensive EDA with visualizations\n              * Create data loaders for PyTorch\n              * Handle video augmentation\n              * Quality control and dataset documentation\n            - Skills: Python, OpenCV, FFmpeg, Data Analysis, Visualization\n            - Deliverables: Clean datasets, EDA report, preprocessing scripts\n\n            Student 2: Deep Learning & Model Development Specialist\n            - Responsibilities:\n              * Implement baseline models (2D CNN + LSTM, 3D CNN)\n              * Train and optimize models\n              * Hyperparameter tuning and experiment tracking\n              * Model evaluation and comparison\n              * Create training pipelines\n              * Performance benchmarking\n            - Skills: PyTorch, Deep Learning, GPU Computing, Experiment Design\n            - Deliverables: Trained models, training scripts, benchmark results\n\n            Student 3: Advanced Architectures & Deployment Specialist\n            - Responsibilities:\n              * Implement video transformers (ViViT, TimeSformer)\n              * Create attention visualization tools\n              * Build real-time classification demo\n              * Develop web application (Streamlit)\n              * Feature extraction and embedding analysis\n              * Model deployment and optimization\n            - Skills: PyTorch, Transformers, Web Development, Visualization\n            - Deliverables: Transformer models, visualization tools, web app, demos\n\n            SHARED RESPONSIBILITIES (All Students):\n            - Weekly team meetings for integration and planning\n            - Collaborative paper writing (divided by sections)\n            - Code reviews and documentation\n            - Experiment result discussion and analysis\n            - Presentation preparation\n            - GitHub repository maintenance\n\n            COMMUNICATION STRUCTURE:\n            - Weekly progress meetings (90 minutes)\n            - Daily standups (15 minutes via Slack/Discord)\n            - Shared experiment tracking (Weights & Biases)\n            - Google Docs for collaborative writing\n            - GitHub for code collaboration with PR reviews\n            - Notion/Trello for project management\n\n            FOR 2 STUDENTS:\n            - Student 1: Data + EDA + Baselines + Traditional 3D CNNs\n            - Student 2: Video Transformers + Visualization + Applications + Deployment\n\n            FOR 4 STUDENTS (If Available):\n            - Student 4: Evaluation & Analysis Specialist\n              * Cross-dataset evaluation\n              * Statistical significance testing\n              * Ablation studies\n              * Error analysis\n              * Benchmark comparisons with SOTA\n              * Result visualization and paper figures\n            ",
  "Research Contributions": "\n            This project offers multiple avenues for impactful research contributions:\n\n            1. METHODOLOGICAL CONTRIBUTIONS:\n            - Comprehensive benchmark study comparing 6+ architectures\n            - Efficient video transformers with sparse attention\n            - Novel temporal attention mechanisms\n            - Transfer learning strategies for video\n\n            2. EMPIRICAL CONTRIBUTIONS:\n            - Extensive ablation studies on frame sampling, resolution, architectures\n            - Temporal pattern analysis showing what models learn\n            - Robustness evaluation against compression and degradation\n            - Cross-dataset generalization study\n\n            3. FEATURE EXTRACTION CONTRIBUTIONS:\n            - Pre-trained video features for all major datasets\n            - Video embeddings analysis and visualization\n            - Action localization using temporal class activation maps\n\n            4. INTERPRETABILITY CONTRIBUTIONS:\n            - Attention visualization framework for video\n            - Human evaluation of attention quality\n            - Counterfactual analysis for video classification\n\n            5. PRACTICAL APPLICATION CONTRIBUTIONS:\n            - Open-source toolkit with pre-trained models\n            - Real-time webcam demo\n            - Video analysis tools (highlights, summarization, retrieval)\n\n            PUBLICATION TARGETS:\n            - CVPR, ICCV, ECCV (computer vision)\n            - NeurIPS, ICML (machine learning)\n            - ACM Multimedia (video understanding)\n            - Workshops and journals\n\n            EXPECTED OUTCOMES:\n            - 1 conference paper\n            - 1 GitHub repository (500+ stars)\n            - Pre-trained models\n            - Web demo\n            - Blog post\n            ",
  "Possible Issues": "\n            TECHNICAL CHALLENGES:\n            1. Computational resources (3D CNNs, transformers are memory-intensive)\n            2. Dataset size (Kinetics-400 is 450 GB)\n            3. Video loading speed (I/O bottleneck)\n            4. Training time (video models are slow)\n            5. Overfitting (video datasets smaller than image datasets)\n\n            SOLUTIONS:\n            - Use gradient accumulation, mixed precision training\n            - Start with UCF101, use Kinetics-Mini\n            - Pre-extract frames, use faster libraries (decord)\n            - Use pre-trained weights, transfer learning\n            - Strong augmentation, regularization, early stopping\n\n            See complete issue list and mitigation strategies in full proposal.\n            ",
  "Additional Resources": "\n            DATASET DOWNLOAD LINKS:\n\n            1. UCF101: https://www.kaggle.com/datasets/matthewjansen/ucf101-action-recognition\n            2. HMDB51: https://serre-lab.clps.brown.edu/resource/hmdb-a-large-human-motion-database/\n            3. Kinetics-400-Mini: https://www.kaggle.com/datasets/shivamb/kinetics-400-mini\n            4. Something-Something V2: https://developer.qualcomm.com/software/ai-datasets/something-something\n\n            PRE-TRAINED MODELS:\n            - torchvision: r3d_18, r2plus1d_18, mc3_18\n            - Third-party: I3D, C3D, SlowFast, TimeSformer, ViViT, X3D\n\n            LIBRARIES:\n            - torch, torchvision, opencv-python, decord\n            - transformers, timm, einops\n            - matplotlib, seaborn, tqdm, wandb\n\n            See complete resources in full proposal document.\n            ",
  "Proposed by": "Dr. Amir Jafari",
  "Proposed by email": "ajafari@gwu.edu",
  "instructor": "Amir Jafari",
  "instructor_email": "ajafari@gwu.edu",
  "collaborator": "None",
  "funding_opportunity": "Open Source Community Project / NSF CAREER / Industry Partnerships",
  "github_repo": "https://github.com/amir-jafari"
}