{
  "Version": "5",
  "Year": "2026",
  "Semester": "Spring",
  "project_name": "Deep Learning for EEG-based Seizure Detection: Multi-Modal Analysis, Temporal Modeling, and Clinical Deployment",
  "Objective": "\n            The goal of this project is to develop a comprehensive, clinically-viable seizure detection system \n            using deep learning on EEG signals. This project will explore state-of-the-art architectures for \n            time-series analysis, implement multiple modeling techniques, and create an end-to-end pipeline \n            from raw EEG data to real-time seizure prediction with clinical deployment capabilities.\n\n            Key Objectives:\n            1. Build a robust EEG preprocessing and feature extraction pipeline that can:\n               - Parse multiple EDF/EDF+ formats (European Data Format)\n               - Handle various EEG montages (bipolar, referential, average reference)\n               - Perform artifact removal (eye blinks, muscle artifacts, electrode noise)\n               - Extract time-domain features (amplitude, variance, line length, energy)\n               - Extract frequency-domain features (band power, spectral entropy, peak frequency)\n               - Generate time-frequency representations (spectrograms, wavelets, Hilbert-Huang)\n               - Handle missing channels and variable sampling rates\n               - Normalize and standardize across patients and recording sessions\n\n            2. Develop comprehensive EEG exploratory data analysis (EDA) toolkit:\n               - Statistical analysis: seizure duration, inter-ictal period, channel statistics\n               - Temporal analysis: seizure onset patterns, pre-ictal signatures, post-ictal recovery\n               - Spectral analysis: power spectral density, band power ratios, coherence\n               - Spatial analysis: seizure propagation patterns, channel correlations, seizure localization\n               - Patient-level analysis: seizure frequency, medication effects, circadian patterns\n               - Annotation quality assessment: expert agreement, label distribution, edge cases\n\n            3. Implement and compare multiple seizure detection architectures:\n               - Traditional Machine Learning (Baseline):\n                 * Random Forest with hand-crafted features\n                 * SVM with spectral and temporal features\n                 * XGBoost with feature engineering\n                 * Logistic Regression with statistical features\n\n               - Deep Learning - CNNs for Spectral Analysis:\n                 * 1D CNN on raw EEG signals\n                 * 2D CNN on spectrograms/wavelets\n                 * ResNet-1D for temporal patterns\n                 * EfficientNet on time-frequency images\n                 * Inception networks for multi-scale features\n\n               - Deep Learning - RNNs for Temporal Dependencies:\n                 * LSTM for long-term dependencies\n                 * GRU (faster alternative to LSTM)\n                 * Bidirectional LSTM/GRU\n                 * Stacked LSTM with attention\n                 * CNN-LSTM hybrid (spatial + temporal)\n\n               - Deep Learning - Transformers:\n                 * Vanilla Transformer for EEG sequences\n                 * BERT-style pre-training on EEG (EEG-BERT)\n                 * GPT-style autoregressive modeling\n                 * Vision Transformer (ViT) on spectrograms\n                 * TimesNet for multi-scale temporal analysis\n                 * Temporal Fusion Transformer (TFT)\n                 * Informer for long sequence modeling\n\n               - Graph Neural Networks (GNNs):\n                 * Graph Convolutional Networks (GCN) for electrode connectivity\n                 * GraphSAGE for scalable graph learning\n                 * Temporal Graph Networks for evolving brain networks\n\n               - Hybrid Architectures:\n                 * CNN + Transformer (ConvNeXt + attention)\n                 * LSTM + Attention mechanisms\n                 * Multi-branch networks (raw + spectral + graph)\n                 * Ensemble models combining multiple approaches\n\n            4. Advanced modeling techniques:\n               - Multi-task learning (seizure detection + type classification)\n               - Transfer learning across patients and datasets\n               - Self-supervised pre-training (masked signal modeling, contrastive learning)\n               - Few-shot learning for rare seizure types\n               - Continual learning for patient-specific adaptation\n               - Uncertainty quantification (Bayesian deep learning, MC Dropout)\n               - Explainable AI (attention visualization, saliency maps, SHAP)\n\n            5. Seizure prediction and early warning:\n               - Pre-ictal state detection (minutes before seizure)\n               - Real-time seizure onset detection\n               - Seizure propagation tracking\n               - Post-ictal state monitoring\n               - False alarm suppression\n               - Patient-specific model fine-tuning\n\n            6. Build practical clinical applications:\n               - Real-time monitoring dashboard\n               - Automated seizure logging and reporting\n               - EEG quality assessment\n               - Medication efficacy tracking\n               - Sleep stage classification (related task)\n               - Anomaly detection for rare events\n\n            7. Create interactive visualization and analysis tools:\n               - Multi-channel EEG viewer with annotations\n               - Spectrogram and time-frequency visualizations\n               - Attention heatmaps showing model focus\n               - Seizure propagation animations\n               - Model performance dashboards\n               - Patient history and trends\n               - Confusion matrices and ROC curves\n\n            8. Deploy as accessible clinical tool:\n               - Web application for clinician review\n               - REST API for hospital integration\n               - Real-time processing pipeline\n               - HIPAA-compliant data handling\n               - Comprehensive documentation\n               - Clinical validation protocols\n            ",
  "Dataset": "\n            All datasets are publicly available for research purposes with proper attribution:\n\n            PRIMARY DATASETS (Seizure Detection):\n\n            1. CHB-MIT Scalp EEG Database (RECOMMENDED FOR BEGINNERS):\n               - URL: https://physionet.org/content/chbmit/1.0.0/\n               - Alternative: https://www.kaggle.com/datasets/adibadea/chbmitseizuredataset\n               - Size: 23 pediatric patients, ~900 hours of continuous EEG, ~10 GB\n               - Channels: 23 channels (10-20 system)\n               - Sampling Rate: 256 Hz\n               - Format: EDF (European Data Format)\n               - Annotations: 198 seizure events with precise timestamps\n               - Content: Continuous scalp EEG from patients with intractable seizures\n               - Duration: Each file 1+ hours\n               - Classes: Seizure vs. non-seizure (binary)\n               - Download: Direct from PhysioNet or Kaggle\n               - Time: 30-45 minutes\n               - Paper: https://doi.org/10.1109/TBME.2010.2053040\n               - Clinical Context: Pediatric epilepsy monitoring unit recordings\n               - Quality: High-quality research-grade EEG\n\n            2. TUH EEG Seizure Corpus (TUSZ) (Large-Scale):\n               - URL: https://isip.piconepress.com/projects/tuh_eeg/html/downloads.shtml\n               - Size: 5,499 sessions from 592 patients, ~650 hours, ~130 GB\n               - Channels: Variable (usually 20-26 channels)\n               - Sampling Rate: 250 Hz or 400 Hz\n               - Format: EDF\n               - Annotations: Multi-annotator consensus labels\n               - Content: Clinical EEG from Temple University Hospital\n               - Seizure Types: Multiple types (focal, generalized, absence, etc.)\n               - Classes: Background, seizure (with subtypes)\n               - Download: Free registration required\n               - Time: 2-3 hours\n               - Paper: https://doi.org/10.3389/fninf.2019.00083\n               - Clinical Context: Diverse patient population, real clinical data\n               - Quality: Variable (reflects real-world clinical recordings)\n\n            3. Siena Scalp EEG Database:\n               - URL: https://physionet.org/content/siena-scalp-eeg/1.0.0/\n               - Size: 14 patients, ~130 hours, ~5 GB\n               - Channels: 29-31 channels\n               - Sampling Rate: 512 Hz\n               - Format: EDF\n               - Annotations: Detailed seizure and artifact labels\n               - Content: Long-term monitoring in epilepsy unit\n               - Seizure count: 47 seizure recordings\n               - Download: Direct from PhysioNet\n               - Time: 20-30 minutes\n               - Paper: https://doi.org/10.1016/j.dib.2020.106058\n\n            4. EEG Motor Movement/Imagery Dataset (Multi-task):\n               - URL: https://physionet.org/content/eegmmidb/1.0.0/\n               - Size: 109 subjects, 1,500+ recordings, ~25 GB\n               - Channels: 64 channels\n               - Sampling Rate: 160 Hz\n               - Format: EDF\n               - Content: Motor tasks and imagery (not seizure, but useful for transfer learning)\n               - Use Case: Pre-training, healthy baseline comparison\n               - Download: Direct from PhysioNet\n               - Time: 45-60 minutes\n\n            5. Bonn University EEG Database (Classic Benchmark):\n               - URL: http://epileptologie-bonn.de/cms/front_content.php?idcat=193\n               - Alternative: https://www.kaggle.com/datasets/harunshimanto/epileptic-seizure-recognition\n               - Size: 500 segments, 5 subjects, ~100 MB\n               - Channels: Single-channel\n               - Sampling Rate: 173.61 Hz\n               - Duration: 23.6 seconds per segment\n               - Format: TXT files\n               - Classes: 5 classes (healthy eyes open/closed, epileptic zone, ictal)\n               - Download: Direct download or Kaggle\n               - Time: 5 minutes\n               - Paper: https://doi.org/10.1088/0031-9155/46/2/301\n               - Note: Small but widely cited, good for quick prototyping\n\n            6. EPILEPSIAE Database (European Consortium):\n               - URL: http://www.epilepsiae.eu/project_outputs/european_database_on_epilepsy\n               - Size: 30 patients, 3,750+ hours, ~500 GB\n               - Channels: Up to 128 intracranial + 32 scalp\n               - Format: Multiple formats\n               - Download: Requires application approval (research use)\n               - Time: Several days\n               - Note: Contains both scalp and intracranial EEG\n\n            7. Kaggle Epileptic Seizure Recognition:\n               - URL: https://www.kaggle.com/c/seizure-prediction\n               - Size: Pre-processed features from 3 dogs and 2 humans\n               - Format: MAT files (MATLAB)\n               - Content: 1-second segments with extracted features\n               - Use Case: Quick start with pre-processed data\n               - Download: Kaggle competition data\n               - Time: 10 minutes\n\n\n            RELATED DATASETS (For Transfer Learning and Multi-task Learning):\n\n            8. Sleep-EDF Database:\n               - URL: https://physionet.org/content/sleep-edf/1.0.0/\n               - Size: 197 whole-night PSG recordings\n               - Use Case: Sleep stage classification, domain adaptation\n               - Channels: EEG, EOG, EMG\n               - Download: PhysioNet\n               - Time: 20 minutes\n\n            9. MIT-BIH Polysomnographic Database:\n               - URL: https://physionet.org/content/slpdb/1.0.0/\n               - Size: 18 recordings, multiple channels\n               - Use Case: Multi-modal learning (EEG + ECG + respiratory)\n\n\n            PRE-PROCESSED FEATURES (For Faster Prototyping):\n\n            10. Extracted EEG Features:\n                - Several Kaggle datasets provide pre-extracted features\n                - Spectral features, statistical features, wavelet coefficients\n                - Use Case: Skip signal processing, focus on modeling\n                - Size: ~100 MB - 1 GB\n                - Format: CSV, NumPy arrays\n\n\n            DATASET SELECTION GUIDE:\n\n            Beginner (Start Here):\n            - CHB-MIT (10 GB, 23 patients, well-documented) - RECOMMENDED\n            - Bonn (100 MB, 5 patients, classic benchmark)\n\n            Intermediate:\n            - Siena (5 GB, high sampling rate, detailed annotations)\n            - TUH Seizure (subset: 20-30 GB)\n\n            Advanced:\n            - TUH Seizure Full (130 GB, largest public dataset)\n            - EPILEPSIAE (500 GB, requires approval)\n\n\n            RECOMMENDED STARTING POINT:\n            1. Start with CHB-MIT (10 GB) - it's manageable, well-documented, and has \n               high-quality annotations\n            2. Prototype on Bonn dataset (100 MB) for rapid iteration\n            3. Scale to TUH Seizure for robustness and generalization\n            4. Validate on Siena for different patient population\n\n\n            DATA CHARACTERISTICS SUMMARY:\n            - Total accessible data: ~150 GB (without EPILEPSIAE)\n            - Patients: 600+ unique patients across datasets\n            - Seizures: 500+ annotated seizure events\n            - Recording hours: 1,500+ hours total\n            - Sampling rates: 160-512 Hz\n            - Channel counts: 1-64 channels (scalp), up to 128 (intracranial)\n            ",
  "Rationale": "\n            Epilepsy affects over 50 million people worldwide, with approximately 30% having drug-resistant \n            epilepsy. Automated seizure detection and prediction has the potential to dramatically improve \n            patient safety, enable timely intervention, and reduce the burden on healthcare providers.\n\n            WHY THIS PROJECT IS TIMELY AND HIGHLY PUBLISHABLE:\n\n            1. TRANSFORMERS ARE REVOLUTIONIZING TIME-SERIES ANALYSIS:\n               - Recent success of transformers in NLP and vision\n               - Limited exploration of transformers for EEG analysis\n               - Self-attention reveals interpretable temporal patterns in brain signals\n               - Opportunity to adapt BERT, GPT architectures to medical time-series\n               - Novel architectures like TimesNet, Informer showing promise\n               - Multi-modal fusion (EEG + clinical data) using cross-attention\n\n            2. CLINICAL DEPLOYMENT GAP:\n               - Many research papers but few deployed systems\n               - Real-time constraints are rarely addressed\n               - Robustness to electrode artifacts and patient variability understudied\n               - This project bridges research and clinical practice\n               - HIPAA-compliant deployment considerations\n               - Clinician-in-the-loop design for trust and adoption\n\n            3. EXPLAINABILITY IS CRITICAL FOR MEDICAL AI:\n               - Black-box models not acceptable in clinical settings\n               - Attention visualization shows which EEG segments trigger predictions\n               - Saliency maps highlight critical frequency bands and channels\n               - SHAP values quantify feature importance\n               - Counterfactual analysis for \"what-if\" scenarios\n               - Clinician validation of model reasoning\n\n            4. PATIENT-SPECIFIC ADAPTATION:\n               - Seizure manifestations vary greatly across patients\n               - Transfer learning from population models to individual patients\n               - Few-shot learning for patients with limited seizure data\n               - Continual learning as patient condition evolves\n               - Meta-learning for rapid personalization\n               - Active learning to select most informative samples for labeling\n\n            5. MULTI-DATASET BENCHMARKING IS RARE:\n               - Most papers evaluate on single dataset\n               - Cross-dataset generalization poorly studied\n               - Opportunity for comprehensive benchmark across CHB-MIT, TUH, Siena, Bonn\n               - Domain adaptation techniques to improve robustness\n               - Dataset-specific challenges and characteristics\n\n            6. REAL-TIME PREDICTION (NOT JUST DETECTION):\n               - Most work focuses on seizure detection (during seizure)\n               - Seizure prediction (minutes before onset) is the holy grail\n               - Pre-ictal state detection enables preventive intervention\n               - Early warning systems for patients and caregivers\n               - Closed-loop therapeutic devices (responsive neurostimulation)\n\n            7. PRACTICAL APPLICATIONS WITH HIGH IMPACT:\n\n               Clinical Applications:\n               - Epilepsy monitoring units: reduce need for 24/7 human monitoring\n               - Ambulatory monitoring: detect seizures in daily life\n               - Medication management: track efficacy, optimize dosing\n               - Surgical planning: identify seizure focus, predict outcomes\n               - Emergency response: automatic alerts to caregivers\n\n               Research Applications:\n               - Drug trials: automated seizure counting, efficacy assessment\n               - Seizure phenotyping: automated classification of seizure types\n               - Brain connectivity analysis: seizure propagation patterns\n               - Biomarker discovery: pre-ictal signatures, predictive features\n\n               Patient Quality of Life:\n               - Independence: patients can live more freely with monitoring\n               - Safety: alerts prevent injuries during seizures\n               - Anxiety reduction: knowing help is automatic\n               - Better treatment: data-driven medication adjustments\n\n\n            8. PUBLICATION VENUES (STRONG ACCEPTANCE FOR QUALITY WORK):\n\n               Top Medical AI Conferences:\n               - MIDL (Medical Imaging with Deep Learning) - dedicated medical AI\n               - MICCAI (Medical Image Computing and Computer Assisted Intervention)\n               - CHIL (Conference on Health, Inference, and Learning)\n               - ML4H (Machine Learning for Healthcare) - NeurIPS workshop\n\n               Biomedical Engineering Conferences:\n               - EMBC (Engineering in Medicine and Biology Conference)\n               - BHI (Biomedical and Health Informatics)\n\n               Machine Learning Conferences:\n               - NeurIPS (Medical AI track)\n               - ICML (Healthcare applications)\n               - ICLR (Time-series and health tracks)\n               - AAAI (AI for Social Impact)\n\n               Neuroscience & Clinical Neurology:\n               - Society for Neuroscience (SfN)\n               - American Epilepsy Society (AES)\n               - International League Against Epilepsy (ILAE)\n\n               Signal Processing:\n               - ICASSP (International Conference on Acoustics, Speech and Signal Processing)\n               - EUSIPCO (European Signal Processing Conference)\n               - IEEE Signal Processing in Medicine and Biology Symposium\n\n               Journals:\n               - Nature Medicine / Nature Biomedical Engineering (high-impact)\n               - IEEE Transactions on Biomedical Engineering (TBME)\n               - NeuroImage: Clinical\n               - Epilepsia (clinical epilepsy journal)\n               - Journal of Neural Engineering\n               - Clinical Neurophysiology\n               - Epilepsy & Behavior\n               - Frontiers in Neuroscience\n\n\n            NOVELTY AND CONTRIBUTION OPPORTUNITIES:\n\n            Students can contribute by:\n            1. Comprehensive transformer benchmark for EEG seizure detection\n            2. Multi-dataset evaluation (CHB-MIT + TUH + Siena + Bonn)\n            3. Patient-specific transfer learning and few-shot adaptation\n            4. Explainable AI framework with attention visualization\n            5. Real-time deployment pipeline with clinical validation\n            6. Pre-ictal detection using self-supervised pre-training\n            7. Multi-task learning (detection + prediction + classification)\n            8. Graph neural networks for EEG channel connectivity\n            9. Uncertainty quantification for reliable clinical decisions\n            10. Open-source toolkit with pre-trained models\n\n\n            RESEARCH QUESTIONS TO EXPLORE:\n\n            1. How do transformers compare to CNNs and RNNs for seizure detection?\n            2. What temporal patterns do attention mechanisms learn in EEG?\n            3. Can self-supervised pre-training improve few-shot seizure detection?\n            4. How well do models generalize across different patient populations?\n            5. What are the optimal input representations (raw, spectrogram, wavelet)?\n            6. Can we reliably predict seizures minutes before onset?\n            7. How do we quantify and communicate model uncertainty to clinicians?\n            8. What are the critical features and frequency bands for seizure detection?\n            9. How can we make models robust to electrode artifacts and missing data?\n            10. What is the minimum viable EEG setup (fewer channels) for accurate detection?\n\n\n            BROADER IMPACT:\n\n            - Patient Safety: Prevent injuries from undetected seizures\n            - Healthcare Efficiency: Reduce clinician workload in monitoring\n            - Research Acceleration: Automated analysis of large EEG datasets\n            - Accessibility: Enable epilepsy monitoring in resource-limited settings\n            - Personalized Medicine: Patient-specific models for precision treatment\n            - Quality of Life: Give patients independence and peace of mind\n            - Scientific Understanding: Reveal seizure mechanisms through interpretable AI\n            - Technology Transfer: Methods applicable to other neurological conditions\n            ",
  "Approach": "\n            PHASE 1: EEG DATA PIPELINE & EXPLORATORY DATA ANALYSIS (Weeks 1-2)\n\n            Week 1: Data Acquisition and Understanding\n            - Download CHB-MIT dataset (10 GB) from PhysioNet/Kaggle\n            - Download Bonn dataset (100 MB) for quick prototyping\n            - Parse EDF files using MNE-Python or PyEDFlib\n            - Understand EEG channel naming (10-20 system: Fp1, Fp2, F3, F4, etc.)\n            - Extract metadata: sampling rate, number of channels, recording duration\n            - Parse seizure annotations and timestamps\n            - Visualize example seizure vs. non-seizure segments\n            - Understand data imbalance (seizures are rare events)\n\n            Code deliverable:\n            ```python\n            # data_loader.py\n            import mne\n            import pyedflib\n            import pandas as pd\n            import numpy as np\n\n            class EEGDataLoader:\n                def load_edf(self, filepath):\n                    # Load EDF file\n                    raw = mne.io.read_raw_edf(filepath, preload=True)\n                    return raw\n\n                def parse_annotations(self, annotation_file):\n                    # Extract seizure start/end times\n                    annotations = pd.read_csv(annotation_file)\n                    return annotations\n\n                def segment_data(self, raw, window_size=4, overlap=2):\n                    # Create overlapping windows\n                    segments = []\n                    labels = []\n                    # ... implementation\n                    return segments, labels\n            ```\n\n            Week 2: Advanced EEG Preprocessing and Feature Engineering\n            Preprocessing steps:\n            - Band-pass filtering (0.5-50 Hz) to remove DC drift and high-frequency noise\n            - Notch filtering (60 Hz in US, 50 Hz in Europe) for power line noise\n            - Artifact removal:\n              * Independent Component Analysis (ICA) for eye blinks, ECG\n              * Wavelet-based denoising\n              * Outlier removal (amplitude thresholds)\n            - Re-referencing (common average reference, bipolar montage)\n            - Normalization (z-score, min-max, robust scaling)\n\n            Feature extraction:\n            Time-domain features:\n            - Statistical: mean, variance, skewness, kurtosis, range\n            - Complexity: Hjorth parameters, line length, energy\n            - Nonlinear: approximate entropy, sample entropy, fractal dimension\n\n            Frequency-domain features:\n            - Band power: Delta (0.5-4 Hz), Theta (4-8 Hz), Alpha (8-13 Hz), \n                          Beta (13-30 Hz), Gamma (30-50 Hz)\n            - Spectral entropy, edge frequency, peak frequency\n            - Power ratios (theta/alpha, delta/theta, etc.)\n\n            Time-frequency features:\n            - Short-time Fourier Transform (STFT) \u2192 spectrograms\n            - Continuous Wavelet Transform (CWT)\n            - Hilbert-Huang Transform (HHT)\n            - Empirical Mode Decomposition (EMD)\n\n            Code deliverable:\n            ```python\n            # feature_extraction.py\n            import scipy.signal as signal\n            from scipy.stats import skew, kurtosis\n            import pywt\n\n            class FeatureExtractor:\n                def extract_time_features(self, eeg_segment):\n                    features = {\n                        'mean': np.mean(eeg_segment, axis=1),\n                        'std': np.std(eeg_segment, axis=1),\n                        'skewness': skew(eeg_segment, axis=1),\n                        'kurtosis': kurtosis(eeg_segment, axis=1),\n                        'line_length': np.sum(np.abs(np.diff(eeg_segment, axis=1)), axis=1)\n                    }\n                    return features\n\n                def extract_frequency_features(self, eeg_segment, fs=256):\n                    # Compute power spectral density\n                    freqs, psd = signal.welch(eeg_segment, fs=fs)\n\n                    # Band power\n                    delta = self.band_power(freqs, psd, 0.5, 4)\n                    theta = self.band_power(freqs, psd, 4, 8)\n                    alpha = self.band_power(freqs, psd, 8, 13)\n                    beta = self.band_power(freqs, psd, 13, 30)\n                    gamma = self.band_power(freqs, psd, 30, 50)\n\n                    return {'delta': delta, 'theta': theta, 'alpha': alpha,\n                            'beta': beta, 'gamma': gamma}\n\n                def create_spectrogram(self, eeg_segment, fs=256):\n                    f, t, Sxx = signal.spectrogram(eeg_segment, fs=fs)\n                    return Sxx\n            ```\n\n            Exploratory Data Analysis (EDA):\n            - Distribution of seizure vs. non-seizure segments\n            - Seizure duration statistics (mean, median, min, max)\n            - Inter-ictal period analysis\n            - Channel-wise statistics and correlations\n            - Frequency band power during seizure vs. baseline\n            - Spectral analysis: plot average spectrogram for seizure/non-seizure\n            - Temporal evolution: how signals change before, during, after seizure\n            - Patient-specific patterns: variability across patients\n\n            Visualizations:\n            - Raw EEG traces (multi-channel plot)\n            - Spectrograms and time-frequency plots\n            - Power spectral density comparisons\n            - Correlation matrices between channels\n            - t-SNE/UMAP embeddings of features\n            - Seizure onset patterns (averaged across events)\n\n            Code deliverable:\n            ```python\n            # eda_visualizer.py\n            import matplotlib.pyplot as plt\n            import seaborn as sns\n\n            class EDAVisualizer:\n                def plot_raw_eeg(self, eeg_data, seizure_start=None, seizure_end=None):\n                    # Multi-channel time-series plot\n                    fig, axes = plt.subplots(eeg_data.shape[0], 1, figsize=(15, 10))\n                    for i, ax in enumerate(axes):\n                        ax.plot(eeg_data[i])\n                        if seizure_start:\n                            ax.axvspan(seizure_start, seizure_end, alpha=0.3, color='red')\n                        ax.set_ylabel(f'Ch {i+1}')\n                    plt.tight_layout()\n\n                def plot_spectrogram(self, eeg_channel, fs=256):\n                    f, t, Sxx = signal.spectrogram(eeg_channel, fs=fs)\n                    plt.pcolormesh(t, f, 10 * np.log10(Sxx), shading='gouraud')\n                    plt.ylabel('Frequency [Hz]')\n                    plt.xlabel('Time [sec]')\n                    plt.colorbar(label='Power [dB]')\n\n                def plot_band_power_comparison(self, seizure_features, normal_features):\n                    # Box plots comparing band powers\n                    bands = ['delta', 'theta', 'alpha', 'beta', 'gamma']\n                    fig, axes = plt.subplots(1, 5, figsize=(20, 4))\n                    for i, band in enumerate(bands):\n                        axes[i].boxplot([normal_features[band], seizure_features[band]])\n                        axes[i].set_xticklabels(['Normal', 'Seizure'])\n                        axes[i].set_title(f'{band.capitalize()} Band')\n            ```\n\n\n            PHASE 2: BASELINE MODELS - TRADITIONAL MACHINE LEARNING (Weeks 3-4)\n\n            Week 3: Classical ML Baselines\n            Implement traditional machine learning approaches:\n\n            1. Random Forest:\n            - Input: Hand-crafted features (time + frequency domain)\n            - Architecture: 100-500 trees, max depth tuning\n            - Features: 50-100 statistical and spectral features per channel\n            - Advantage: Interpretable, fast, good baseline\n\n            2. Support Vector Machine (SVM):\n            - Input: Same hand-crafted features\n            - Kernels: Linear, RBF, polynomial\n            - Feature scaling critical\n            - Advantage: Strong performance with limited data\n\n            3. XGBoost:\n            - Gradient boosting on features\n            - Hyperparameter tuning: learning rate, max depth, n_estimators\n            - Feature importance analysis\n            - Advantage: State-of-art for tabular data\n\n            Code deliverable:\n            ```python\n            # traditional_ml.py\n            from sklearn.ensemble import RandomForestClassifier\n            from sklearn.svm import SVC\n            import xgboost as xgb\n            from sklearn.model_selection import cross_val_score\n\n            class TraditionalMLPipeline:\n                def __init__(self):\n                    self.models = {\n                        'rf': RandomForestClassifier(n_estimators=200),\n                        'svm': SVC(kernel='rbf', probability=True),\n                        'xgb': xgb.XGBClassifier(n_estimators=200)\n                    }\n\n                def train_evaluate(self, X_train, y_train, X_test, y_test):\n                    results = {}\n                    for name, model in self.models.items():\n                        model.fit(X_train, y_train)\n                        y_pred = model.predict(X_test)\n                        results[name] = self.compute_metrics(y_test, y_pred)\n                    return results\n            ```\n\n            Week 4: Deep Learning Baseline - 1D CNN\n            First deep learning approach:\n\n            Architecture:\n            ```\n            Input: Raw EEG (n_channels, sequence_length)\n            \u2193\n            Conv1D(64, kernel=7) \u2192 BatchNorm \u2192 ReLU \u2192 MaxPool\n            \u2193\n            Conv1D(128, kernel=5) \u2192 BatchNorm \u2192 ReLU \u2192 MaxPool\n            \u2193\n            Conv1D(256, kernel=3) \u2192 BatchNorm \u2192 ReLU \u2192 MaxPool\n            \u2193\n            Flatten \u2192 Dense(512) \u2192 Dropout(0.5) \u2192 Dense(2)\n            ```\n\n            Code deliverable:\n            ```python\n            # cnn_baseline.py\n            import torch\n            import torch.nn as nn\n\n            class CNN1D_Baseline(nn.Module):\n                def __init__(self, n_channels=23, seq_length=1024):\n                    super().__init__()\n                    self.conv1 = nn.Conv1d(n_channels, 64, kernel_size=7, padding=3)\n                    self.bn1 = nn.BatchNorm1d(64)\n                    self.conv2 = nn.Conv1d(64, 128, kernel_size=5, padding=2)\n                    self.bn2 = nn.BatchNorm1d(128)\n                    self.conv3 = nn.Conv1d(128, 256, kernel_size=3, padding=1)\n                    self.bn3 = nn.BatchNorm1d(256)\n\n                    self.pool = nn.MaxPool1d(2)\n                    self.relu = nn.ReLU()\n                    self.dropout = nn.Dropout(0.5)\n\n                    self.fc1 = nn.Linear(256 * (seq_length // 8), 512)\n                    self.fc2 = nn.Linear(512, 2)\n\n                def forward(self, x):\n                    x = self.pool(self.relu(self.bn1(self.conv1(x))))\n                    x = self.pool(self.relu(self.bn2(self.conv2(x))))\n                    x = self.pool(self.relu(self.bn3(self.conv3(x))))\n                    x = x.view(x.size(0), -1)\n                    x = self.dropout(self.relu(self.fc1(x)))\n                    x = self.fc2(x)\n                    return x\n            ```\n\n\n            PHASE 3: RECURRENT NEURAL NETWORKS FOR TEMPORAL MODELING (Weeks 5-6)\n\n            Week 5: LSTM and GRU Networks\n\n            1. Vanilla LSTM:\n            ```python\n            class LSTMSeizureDetector(nn.Module):\n                def __init__(self, input_size=23, hidden_size=256, num_layers=2):\n                    super().__init__()\n                    self.lstm = nn.LSTM(input_size, hidden_size, num_layers, \n                                       batch_first=True, dropout=0.3)\n                    self.fc = nn.Linear(hidden_size, 2)\n\n                def forward(self, x):\n                    # x: (batch, seq_len, channels)\n                    lstm_out, (h_n, c_n) = self.lstm(x)\n                    output = self.fc(h_n[-1])  # Use last hidden state\n                    return output\n            ```\n\n            2. Bidirectional LSTM:\n            - Process sequence forward and backward\n            - Capture both past and future context\n            - Hidden size doubled (256 * 2 = 512)\n\n            3. Attention-based LSTM:\n            ```python\n            class AttentionLSTM(nn.Module):\n                def __init__(self, input_size=23, hidden_size=256):\n                    super().__init__()\n                    self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n                    self.attention = nn.Linear(hidden_size, 1)\n                    self.fc = nn.Linear(hidden_size, 2)\n\n                def forward(self, x):\n                    lstm_out, _ = self.lstm(x)  # (batch, seq, hidden)\n                    attention_weights = torch.softmax(self.attention(lstm_out), dim=1)\n                    context = torch.sum(attention_weights * lstm_out, dim=1)\n                    return self.fc(context)\n            ```\n\n            Week 6: CNN-LSTM Hybrid\n            Combine spatial (CNN) and temporal (LSTM) processing:\n\n            ```python\n            class CNN_LSTM(nn.Module):\n                def __init__(self):\n                    super().__init__()\n                    # CNN for feature extraction\n                    self.conv1 = nn.Conv1d(23, 64, kernel_size=3)\n                    self.conv2 = nn.Conv1d(64, 128, kernel_size=3)\n                    self.pool = nn.MaxPool1d(2)\n\n                    # LSTM for temporal modeling\n                    self.lstm = nn.LSTM(128, 256, num_layers=2, batch_first=True)\n                    self.fc = nn.Linear(256, 2)\n\n                def forward(self, x):\n                    # CNN feature extraction\n                    x = self.pool(F.relu(self.conv1(x)))\n                    x = self.pool(F.relu(self.conv2(x)))\n\n                    # Reshape for LSTM (batch, seq, features)\n                    x = x.permute(0, 2, 1)\n\n                    # LSTM temporal modeling\n                    lstm_out, (h_n, _) = self.lstm(x)\n                    return self.fc(h_n[-1])\n            ```\n\n\n            PHASE 4: TRANSFORMER ARCHITECTURES FOR EEG (Weeks 7-8)\n\n            Week 7: Vanilla Transformer and EEG-Specific Adaptations\n\n            1. Standard Transformer Encoder:\n            ```python\n            class TransformerSeizureDetector(nn.Module):\n                def __init__(self, input_dim=23, d_model=256, nhead=8, \n                            num_layers=6, seq_length=1024):\n                    super().__init__()\n                    self.embedding = nn.Linear(input_dim, d_model)\n                    self.pos_encoder = PositionalEncoding(d_model, seq_length)\n\n                    encoder_layer = nn.TransformerEncoderLayer(\n                        d_model=d_model, nhead=nhead, dim_feedforward=1024\n                    )\n                    self.transformer = nn.TransformerEncoder(encoder_layer, num_layers)\n\n                    self.fc = nn.Linear(d_model, 2)\n\n                def forward(self, x):\n                    # x: (batch, seq_len, channels)\n                    x = self.embedding(x)\n                    x = self.pos_encoder(x)\n                    x = self.transformer(x)\n                    x = x.mean(dim=1)  # Global average pooling\n                    return self.fc(x)\n            ```\n\n            2. EEG-BERT (Inspired by BERT pre-training):\n            - Pre-training task: Masked EEG modeling (mask random time segments)\n            - Fine-tuning: Seizure classification\n            - Advantage: Learn robust EEG representations from unlabeled data\n\n            ```python\n            class EEG_BERT(nn.Module):\n                def __init__(self):\n                    super().__init__()\n                    self.bert_encoder = TransformerEncoder(...)\n\n                def pretrain_step(self, x, mask_indices):\n                    # Mask random segments\n                    masked_x = x.clone()\n                    masked_x[:, mask_indices, :] = 0\n\n                    # Predict masked segments\n                    reconstructed = self.bert_encoder(masked_x)\n                    loss = F.mse_loss(reconstructed[:, mask_indices, :], \n                                     x[:, mask_indices, :])\n                    return loss\n\n                def finetune_step(self, x, labels):\n                    features = self.bert_encoder(x)\n                    logits = self.classifier(features.mean(dim=1))\n                    return logits\n            ```\n\n            Week 8: Advanced Transformers - TimesNet, Informer\n\n            1. TimesNet (Multi-scale Temporal Analysis):\n            - Converts 1D time series to 2D representations\n            - Applies 2D convolutions to capture multi-scale patterns\n            - State-of-art for time-series classification\n\n            2. Informer (Efficient Long Sequence Modeling):\n            - ProbSparse self-attention for long sequences\n            - Reduces complexity from O(L\u00b2) to O(L log L)\n            - Good for long EEG recordings\n\n            3. Temporal Fusion Transformer (TFT):\n            - Multi-horizon forecasting\n            - Variable selection network\n            - Interpretable attention\n\n            Code deliverable:\n            ```python\n            # advanced_transformers.py\n            from timesnet import TimesNet\n            from informer import Informer\n\n            class TimesNetEEG(nn.Module):\n                def __init__(self):\n                    super().__init__()\n                    self.timesnet = TimesNet(\n                        seq_len=1024,\n                        pred_len=1,\n                        enc_in=23,\n                        c_out=2,\n                        d_model=256,\n                        n_heads=8\n                    )\n\n                def forward(self, x):\n                    return self.timesnet(x)\n            ```\n\n\n            PHASE 5: GRAPH NEURAL NETWORKS AND SPATIAL MODELING (Weeks 9-10)\n\n            Week 9: Graph Convolutional Networks for EEG Channels\n\n            Model EEG channels as a graph:\n            - Nodes: EEG electrodes (23 nodes for CHB-MIT)\n            - Edges: Physical proximity or functional connectivity\n            - Node features: EEG signals or extracted features\n\n            ```python\n            import torch_geometric.nn as gnn\n\n            class EEG_GCN(nn.Module):\n                def __init__(self, num_nodes=23, node_features=256):\n                    super().__init__()\n                    # Feature extraction per channel\n                    self.channel_encoder = nn.LSTM(1, node_features, batch_first=True)\n\n                    # Graph convolution layers\n                    self.gcn1 = gnn.GCNConv(node_features, 128)\n                    self.gcn2 = gnn.GCNConv(128, 64)\n\n                    # Classifier\n                    self.fc = nn.Linear(64 * num_nodes, 2)\n\n                def forward(self, x, edge_index):\n                    # x: (batch, num_nodes, seq_len)\n                    batch_size = x.size(0)\n\n                    # Extract features per channel\n                    node_features = []\n                    for i in range(x.size(1)):\n                        _, (h_n, _) = self.channel_encoder(x[:, i:i+1, :].transpose(1, 2))\n                        node_features.append(h_n[-1])\n\n                    x = torch.stack(node_features, dim=1)  # (batch, nodes, features)\n\n                    # Graph convolution\n                    x = x.view(-1, x.size(-1))  # Flatten batch and nodes\n                    x = F.relu(self.gcn1(x, edge_index))\n                    x = F.relu(self.gcn2(x, edge_index))\n\n                    x = x.view(batch_size, -1)  # (batch, nodes * features)\n                    return self.fc(x)\n            ```\n\n            Define graph structure:\n            ```python\n            def create_eeg_graph():\n                # Define electrode positions (10-20 system)\n                electrode_positions = {\n                    'Fp1': (0, 1), 'Fp2': (1, 1),\n                    'F3': (0, 2), 'F4': (1, 2),\n                    # ... all 23 electrodes\n                }\n\n                # Connect spatially adjacent electrodes\n                edges = []\n                for i, (name1, pos1) in enumerate(electrode_positions.items()):\n                    for j, (name2, pos2) in enumerate(electrode_positions.items()):\n                        distance = np.sqrt((pos1[0]-pos2[0])**2 + (pos1[1]-pos2[1])**2)\n                        if distance < 1.5 and i != j:\n                            edges.append([i, j])\n\n                edge_index = torch.tensor(edges).t()\n                return edge_index\n            ```\n\n            Week 10: Temporal Graph Networks and Multi-branch Architectures\n\n            1. Temporal Graph Networks:\n            - Graph structure evolves over time\n            - Capture both spatial and temporal dependencies\n\n            2. Multi-branch Architecture:\n            - Branch 1: Raw signal \u2192 1D CNN\n            - Branch 2: Spectrogram \u2192 2D CNN\n            - Branch 3: Graph \u2192 GCN\n            - Fusion: Concatenate features \u2192 Classifier\n\n            ```python\n            class MultiBranchEEG(nn.Module):\n                def __init__(self):\n                    super().__init__()\n                    # Branch 1: Raw signal processing\n                    self.raw_branch = CNN1D_Baseline()\n\n                    # Branch 2: Spectrogram processing\n                    self.spec_branch = ResNet18_2D()\n\n                    # Branch 3: Graph processing\n                    self.graph_branch = EEG_GCN()\n\n                    # Fusion layer\n                    self.fusion = nn.Linear(512 + 512 + 256, 2)\n\n                def forward(self, raw_eeg, spectrogram, graph_data):\n                    feat1 = self.raw_branch(raw_eeg)\n                    feat2 = self.spec_branch(spectrogram)\n                    feat3 = self.graph_branch(graph_data['x'], graph_data['edge_index'])\n\n                    combined = torch.cat([feat1, feat2, feat3], dim=1)\n                    return self.fusion(combined)\n            ```\n\n\n            PHASE 6: ADVANCED TECHNIQUES AND OPTIMIZATION (Weeks 11-12)\n\n            Week 11: Transfer Learning and Domain Adaptation\n\n            1. Pre-training strategies:\n            - Pre-train on large dataset (TUH) \u2192 Fine-tune on CHB-MIT\n            - Pre-train on sleep-stage classification \u2192 Transfer to seizure detection\n            - Self-supervised pre-training on unlabeled EEG\n\n            2. Patient-specific adaptation:\n            ```python\n            # Transfer learning pipeline\n            def patient_specific_finetuning(pretrained_model, patient_data):\n                # Freeze early layers\n                for param in pretrained_model.conv_layers.parameters():\n                    param.requires_grad = False\n\n                # Fine-tune classifier only\n                optimizer = torch.optim.Adam(\n                    pretrained_model.fc.parameters(), lr=0.0001\n                )\n\n                # Train on patient-specific data (few-shot)\n                for epoch in range(10):\n                    loss = train_epoch(pretrained_model, patient_data, optimizer)\n\n                return pretrained_model\n            ```\n\n            3. Domain adaptation techniques:\n            - Adversarial training for domain-invariant features\n            - Maximum Mean Discrepancy (MMD) loss\n            - Correlation alignment (CORAL)\n\n            Week 12: Uncertainty Quantification and Explainability\n\n            1. Bayesian Deep Learning:\n            ```python\n            class BayesianCNN(nn.Module):\n                def __init__(self):\n                    super().__init__()\n                    self.conv1 = nn.Conv1d(23, 64, 3)\n                    self.dropout1 = nn.Dropout(0.3)\n                    self.conv2 = nn.Conv1d(64, 128, 3)\n                    self.dropout2 = nn.Dropout(0.3)\n                    self.fc = nn.Linear(128, 2)\n\n                def forward(self, x, num_samples=10):\n                    predictions = []\n                    for _ in range(num_samples):\n                        # Stochastic forward pass\n                        out = self.dropout1(F.relu(self.conv1(x)))\n                        out = self.dropout2(F.relu(self.conv2(out)))\n                        out = self.fc(out.mean(dim=2))\n                        predictions.append(out)\n\n                    # Compute mean and variance\n                    predictions = torch.stack(predictions)\n                    mean_pred = predictions.mean(dim=0)\n                    uncertainty = predictions.std(dim=0)\n\n                    return mean_pred, uncertainty\n            ```\n\n            2. Attention Visualization:\n            ```python\n            def visualize_attention(model, eeg_segment):\n                # Get attention weights from transformer\n                attention_weights = model.get_attention_weights(eeg_segment)\n\n                # Plot heatmap\n                plt.figure(figsize=(12, 4))\n                plt.imshow(attention_weights, aspect='auto', cmap='hot')\n                plt.colorbar(label='Attention Weight')\n                plt.xlabel('Time Step')\n                plt.ylabel('Attention Head')\n                plt.title('Temporal Attention Patterns')\n                plt.show()\n            ```\n\n            3. SHAP Values for Feature Importance:\n            ```python\n            import shap\n\n            def explain_prediction(model, eeg_sample):\n                explainer = shap.DeepExplainer(model, background_data)\n                shap_values = explainer.shap_values(eeg_sample)\n\n                # Plot feature importance\n                shap.summary_plot(shap_values, eeg_sample, \n                                 feature_names=channel_names)\n            ```\n\n\n            PHASE 7: CLINICAL DEPLOYMENT AND REAL-TIME SYSTEM (Weeks 13-14)\n\n            Week 13: Real-Time Processing Pipeline\n\n            1. Streaming EEG Processing:\n            ```python\n            class RealTimeSeizureDetector:\n                def __init__(self, model, window_size=4, stride=1):\n                    self.model = model\n                    self.window_size = window_size\n                    self.stride = stride\n                    self.buffer = collections.deque(maxlen=window_size * 256)\n\n                def process_new_sample(self, eeg_sample):\n                    # Add to buffer\n                    self.buffer.append(eeg_sample)\n\n                    # When buffer is full, make prediction\n                    if len(self.buffer) == self.buffer.maxlen:\n                        segment = np.array(self.buffer)\n                        prediction, uncertainty = self.model.predict(segment)\n\n                        if prediction == 'seizure' and uncertainty < 0.1:\n                            self.trigger_alert()\n\n                        # Slide window\n                        for _ in range(self.stride * 256):\n                            self.buffer.popleft()\n\n                def trigger_alert(self):\n                    # Send notification to clinician/caregiver\n                    send_notification(\"Seizure detected!\")\n            ```\n\n            2. Latency Optimization:\n            - Model quantization (FP32 \u2192 FP16 or INT8)\n            - Pruning: Remove low-importance weights\n            - Knowledge distillation: Large model \u2192 Small model\n            - ONNX export for optimized inference\n\n            Week 14: Web Application and Clinical Interface\n\n            Streamlit/Gradio Dashboard:\n            ```python\n            import streamlit as st\n\n            st.title(\"EEG Seizure Detection System\")\n\n            # File upload\n            uploaded_file = st.file_uploader(\"Upload EDF file\", type=['edf'])\n\n            if uploaded_file:\n                # Load and process\n                eeg_data = load_edf(uploaded_file)\n\n                # Display raw EEG\n                st.subheader(\"Raw EEG Signals\")\n                fig = plot_eeg_channels(eeg_data)\n                st.pyplot(fig)\n\n                # Run detection\n                if st.button(\"Detect Seizures\"):\n                    with st.spinner(\"Processing...\"):\n                        predictions, timestamps = model.predict(eeg_data)\n\n                    # Display results\n                    st.subheader(\"Detection Results\")\n                    for i, (pred, time) in enumerate(zip(predictions, timestamps)):\n                        if pred == 1:\n                            st.error(f\"\u26a0\ufe0f Seizure detected at {time}\")\n\n                    # Visualization\n                    st.subheader(\"Attention Heatmap\")\n                    attention_fig = visualize_attention(model, eeg_data)\n                    st.pyplot(attention_fig)\n            ```\n\n\n            EVALUATION METRICS:\n\n            1. Classification Metrics:\n            - Accuracy, Precision, Recall, F1-Score\n            - ROC-AUC, PR-AUC (important for imbalanced data)\n            - Sensitivity (True Positive Rate) - critical for medical application\n            - Specificity (True Negative Rate)\n            - False Positive Rate per hour (FPR/h) - clinical standard\n\n            2. Temporal Metrics:\n            - Detection latency (time from seizure onset to detection)\n            - Prediction horizon (how early can we predict)\n            - Event-based metrics (per seizure, not per segment)\n\n            3. Clinical Metrics:\n            - Patient-level performance (not just segment-level)\n            - Cross-patient generalization\n            - Robustness to electrode artifacts\n\n\n            CODE ORGANIZATION:\n\n            ```\n            eeg-seizure-detection/\n            \u251c\u2500\u2500 data/\n            \u2502   \u251c\u2500\u2500 raw/                 # Original EDF files\n            \u2502   \u251c\u2500\u2500 processed/           # Preprocessed numpy arrays\n            \u2502   \u2514\u2500\u2500 annotations/         # Seizure labels\n            \u251c\u2500\u2500 src/\n            \u2502   \u251c\u2500\u2500 data_processing/\n            \u2502   \u2502   \u251c\u2500\u2500 loader.py\n            \u2502   \u2502   \u251c\u2500\u2500 preprocessor.py\n            \u2502   \u2502   \u2514\u2500\u2500 feature_extractor.py\n            \u2502   \u251c\u2500\u2500 models/\n            \u2502   \u2502   \u251c\u2500\u2500 cnn.py\n            \u2502   \u2502   \u251c\u2500\u2500 lstm.py\n            \u2502   \u2502   \u251c\u2500\u2500 transformer.py\n            \u2502   \u2502   \u251c\u2500\u2500 gnn.py\n            \u2502   \u2502   \u2514\u2500\u2500 ensemble.py\n            \u2502   \u251c\u2500\u2500 training/\n            \u2502   \u2502   \u251c\u2500\u2500 trainer.py\n            \u2502   \u2502   \u2514\u2500\u2500 callbacks.py\n            \u2502   \u251c\u2500\u2500 evaluation/\n            \u2502   \u2502   \u251c\u2500\u2500 metrics.py\n            \u2502   \u2502   \u2514\u2500\u2500 visualizer.py\n            \u2502   \u2514\u2500\u2500 deployment/\n            \u2502       \u251c\u2500\u2500 realtime.py\n            \u2502       \u2514\u2500\u2500 api.py\n            \u251c\u2500\u2500 notebooks/\n            \u2502   \u251c\u2500\u2500 01_EDA.ipynb\n            \u2502   \u251c\u2500\u2500 02_Baseline_Models.ipynb\n            \u2502   \u251c\u2500\u2500 03_Deep_Learning.ipynb\n            \u2502   \u2514\u2500\u2500 04_Transformers.ipynb\n            \u251c\u2500\u2500 configs/\n            \u2502   \u2514\u2500\u2500 model_configs.yaml\n            \u251c\u2500\u2500 tests/\n            \u251c\u2500\u2500 app.py                   # Streamlit app\n            \u251c\u2500\u2500 requirements.txt\n            \u2514\u2500\u2500 README.md\n            ```\n\n\n            EXPERIMENT TRACKING:\n\n            Use Weights & Biases (wandb) for comprehensive tracking:\n            ```python\n            import wandb\n\n            wandb.init(project=\"eeg-seizure-detection\", \n                      config={\n                          \"model\": \"Transformer\",\n                          \"dataset\": \"CHB-MIT\",\n                          \"learning_rate\": 0.001,\n                          \"batch_size\": 32\n                      })\n\n            # Log metrics\n            wandb.log({\n                \"train_loss\": loss,\n                \"val_accuracy\": accuracy,\n                \"val_auc\": auc\n            })\n\n            # Log visualizations\n            wandb.log({\"attention_heatmap\": wandb.Image(fig)})\n            ```\n            ",
  "Timeline": "\n            Week 1:     Dataset Download, EDF Parsing, Initial EDA\n            Week 2:     Advanced Preprocessing, Feature Engineering, Spectral Analysis\n            Week 3:     Traditional ML Baselines (Random Forest, SVM, XGBoost)\n            Week 4:     Deep Learning Baseline (1D CNN)\n            Week 5:     LSTM and GRU Networks\n            Week 6:     CNN-LSTM Hybrid, Bidirectional Networks\n            Week 7:     Transformer Implementation, EEG-BERT\n            Week 8:     Advanced Transformers (TimesNet, Informer, TFT)\n            Week 9:     Graph Neural Networks (GCN for EEG channels)\n            Week 10:    Temporal Graph Networks, Multi-branch Architecture\n            Week 11:    Transfer Learning, Patient-specific Adaptation\n            Week 12:    Uncertainty Quantification, Explainability (SHAP, Attention)\n            Week 13:    Real-time Processing, Deployment Pipeline\n            Week 14:    Web Application, Clinical Interface, Documentation\n\n            TOTAL: 14 weeks (one semester)\n\n            KEY MILESTONES:\n            - Week 2:  Clean datasets, comprehensive EDA complete, preprocessing pipeline working\n            - Week 4:  All baseline models trained (ML + 1D CNN)\n            - Week 6:  RNN models implemented and benchmarked\n            - Week 8:  Transformer models trained and evaluated\n            - Week 10: All architectures complete, multi-dataset evaluation done\n            - Week 12: Explainability tools ready, uncertainty quantification implemented\n            - Week 14: Paper submitted, code released, web demo deployed\n\n            DELIVERABLES BY WEEK 14:\n            - 8-page conference paper (MIDL/MICCAI/CHIL format)\n            - GitHub repository with full implementation\n            - Pre-trained models on Hugging Face/Zenodo\n            - Interactive web demo (Streamlit with real-time capability)\n            - Comprehensive documentation and tutorials\n            - Jupyter notebooks for reproducibility\n            - Clinical validation report\n            - Presentation slides and poster\n            - Blog post explaining methodology and clinical impact\n            - Video demonstrations of real-time system\n            ",
  "Expected Number Students": "\n            RECOMMENDED: 2-3 students\n\n            ROLE DISTRIBUTION FOR 3 STUDENTS:\n\n            Student 1: Data Engineering & Signal Processing Specialist\n            - Responsibilities:\n              * Download and organize EEG datasets (CHB-MIT, TUH, Siena, Bonn)\n              * Parse EDF files and extract metadata\n              * Implement comprehensive preprocessing pipeline\n              * Perform artifact removal (ICA, wavelet denoising)\n              * Extract time, frequency, and time-frequency features\n              * Conduct extensive EDA with visualizations\n              * Create PyTorch data loaders and augmentation\n              * Quality control and dataset documentation\n            - Skills: Python, Signal Processing, MNE-Python, SciPy, Data Analysis\n            - Deliverables: Clean datasets, EDA report, feature extraction library\n\n            Student 2: Deep Learning & Model Development Specialist\n            - Responsibilities:\n              * Implement baseline models (CNN, LSTM, CNN-LSTM)\n              * Implement traditional ML baselines (RF, SVM, XGBoost)\n              * Train and optimize all models\n              * Hyperparameter tuning and experiment tracking (wandb)\n              * Cross-validation and statistical testing\n              * Model evaluation and comparison\n              * Performance benchmarking across datasets\n              * Write training pipelines and callbacks\n            - Skills: PyTorch, Deep Learning, Machine Learning, Experiment Design\n            - Deliverables: All baseline models, training scripts, benchmark results\n\n            Student 3: Advanced Architectures & Deployment Specialist\n            - Responsibilities:\n              * Implement transformer models (vanilla, EEG-BERT, TimesNet, Informer)\n              * Implement graph neural networks for EEG\n              * Transfer learning and domain adaptation\n              * Uncertainty quantification (Bayesian, MC Dropout)\n              * Explainability tools (attention visualization, SHAP)\n              * Real-time processing pipeline\n              * Web application development (Streamlit)\n              * Model deployment and clinical interface\n            - Skills: PyTorch, Transformers, GNNs, Web Development, Deployment\n            - Deliverables: Advanced models, explainability tools, web app, real-time system\n\n            SHARED RESPONSIBILITIES (All Students):\n            - Weekly team meetings for integration and planning\n            - Collaborative paper writing (divided by sections)\n            - Code reviews and documentation\n            - Experiment result discussion and analysis\n            - Clinical consultation (with Dr. Koubeissi)\n            - Presentation preparation\n            - GitHub repository maintenance\n            - Literature review and related work\n\n            COMMUNICATION STRUCTURE:\n            - Weekly progress meetings (90 minutes)\n            - Daily standups (15 minutes via Slack/Discord)\n            - Bi-weekly meetings with Dr. Koubeissi for clinical guidance\n            - Shared experiment tracking (Weights & Biases)\n            - Google Docs for collaborative writing\n            - GitHub for code collaboration with PR reviews\n            - Notion/Trello for project management\n            - Slack channel for quick communication\n\n            FOR 2 STUDENTS:\n            - Student 1: Data + Preprocessing + EDA + Traditional ML + CNNs/RNNs\n            - Student 2: Transformers + GNNs + Explainability + Deployment + Web App\n\n            FOR 4 STUDENTS (If Available):\n            - Student 4: Clinical Validation & Analysis Specialist\n              * Cross-dataset evaluation and generalization study\n              * Statistical significance testing\n              * Clinical metric computation (FPR/h, detection latency)\n              * Error analysis and failure case investigation\n              * Patient-level performance analysis\n              * Robustness testing (artifacts, missing channels)\n              * Comparison with SOTA methods from literature\n              * Result visualization and paper figures\n              * Clinical documentation and user manual\n            ",
  "Research Contributions": "\n            This project offers multiple avenues for impactful research contributions:\n\n            1. METHODOLOGICAL CONTRIBUTIONS:\n            - Comprehensive benchmark of 10+ architectures on EEG seizure detection\n            - Novel transformer adaptations for medical time-series (EEG-BERT)\n            - Graph neural networks for EEG channel connectivity\n            - Multi-branch fusion architecture (raw + spectral + graph)\n            - Self-supervised pre-training strategies for EEG\n            - Patient-specific transfer learning framework\n            - Uncertainty quantification for clinical reliability\n\n            2. EMPIRICAL CONTRIBUTIONS:\n            - Multi-dataset evaluation (CHB-MIT, TUH, Siena, Bonn)\n            - Cross-dataset generalization study\n            - Extensive ablation studies:\n              * Window size (1s, 2s, 4s, 8s)\n              * Sampling rate (downsampling effects)\n              * Number of channels (minimal viable setup)\n              * Input representations (raw, spectrogram, wavelet)\n            - Robustness analysis:\n              * Electrode artifacts and missing channels\n              * Patient variability and age effects\n              * Medication effects\n            - Temporal analysis: detection latency, prediction horizon\n            - Statistical significance testing across all experiments\n\n            3. EXPLAINABILITY CONTRIBUTIONS:\n            - Attention visualization framework for EEG transformers\n            - SHAP values for feature importance in medical context\n            - Saliency maps showing critical time-frequency patterns\n            - Seizure propagation visualization through graph\n            - Human evaluation: clinical validation of attention quality\n            - Counterfactual analysis for model reasoning\n\n            4. CLINICAL IMPACT CONTRIBUTIONS:\n            - Real-time seizure detection system\n            - False positive suppression techniques (critical for clinical adoption)\n            - Patient-specific model adaptation with minimal data\n            - Uncertainty communication for clinical decision support\n            - Integration with existing hospital EEG systems\n            - Clinical validation protocol and results\n\n            5. OPEN-SOURCE CONTRIBUTIONS:\n            - Comprehensive toolkit with pre-trained models\n            - Preprocessing library for EEG (MNE-compatible)\n            - Interactive web application for clinicians\n            - Extensive documentation and tutorials\n            - Reproducible research with all code and configs\n            - Pre-processed datasets for faster research iteration\n\n            PUBLICATION TARGETS:\n\n            Primary Target (Choose 1-2):\n            - MIDL (Medical Imaging with Deep Learning) - June deadline\n            - MICCAI (Medical Image Computing) - March deadline\n            - CHIL (Conference on Health, Inference, and Learning) - January deadline\n            - ML4H @ NeurIPS (Machine Learning for Healthcare) - October deadline\n\n            Alternative Venues:\n            - EMBC (Engineering in Medicine and Biology)\n            - ICASSP (Signal Processing)\n            - Epilepsia Journal (clinical impact)\n            - IEEE TBME (biomedical engineering)\n            - Journal of Neural Engineering\n\n            Workshop Papers (For Interim Results):\n            - NeurIPS Workshop on Medical Imaging\n            - ICLR Workshop on AI for Health\n            - CVPR Workshop on Medical Computer Vision\n\n            EXPECTED OUTCOMES:\n            - 1-2 conference papers\n            - 1 journal paper (extended version)\n            - 1 GitHub repository (target: 200+ stars)\n            - Pre-trained models on Hugging Face\n            - Web demo with 100+ users\n            - Blog post (10,000+ views)\n            - Collaboration with GWU Medical Faculty Advancement\n\n            BROADER IMPACT:\n            - Improve patient safety through automated monitoring\n            - Reduce clinician workload in epilepsy monitoring units\n            - Enable home monitoring for epilepsy patients\n            - Accelerate epilepsy drug trials with automated seizure counting\n            - Provide open-source tools for epilepsy research community\n            - Potential for clinical deployment at GWU Hospital\n            ",
  "Possible Issues": "\n            TECHNICAL CHALLENGES AND SOLUTIONS:\n\n            1. CLASS IMBALANCE (Seizures are rare events):\n               Problem: ~1-5% of EEG contains seizures\n               Solutions:\n               - Weighted loss functions (higher weight for seizure class)\n               - Focal loss to focus on hard examples\n               - Synthetic minority oversampling (SMOTE)\n               - Data augmentation (time warping, jittering, channel dropout)\n               - Use F1-score, ROC-AUC instead of accuracy\n               - Ensemble methods\n\n            2. PATIENT VARIABILITY (Different seizure manifestations):\n               Problem: Models trained on one patient may not generalize\n               Solutions:\n               - Transfer learning: population model \u2192 patient-specific fine-tuning\n               - Meta-learning for rapid adaptation\n               - Domain adaptation techniques\n               - Patient-specific feature normalization\n               - Leave-one-patient-out cross-validation\n               - Multi-task learning across patients\n\n            3. COMPUTATIONAL RESOURCES:\n               Problem: Transformers and GNNs are memory-intensive\n               Solutions:\n               - Start with smaller models, scale up gradually\n               - Use gradient accumulation for larger batch sizes\n               - Mixed precision training (FP16)\n               - Model pruning and quantization\n               - Use Google Colab Pro or university GPU cluster\n               - Efficient attention mechanisms (linear attention)\n               - Distributed training if multiple GPUs available\n\n            4. DATA SIZE:\n               Problem: TUH dataset is 130 GB\n               Solutions:\n               - Start with CHB-MIT (10 GB)\n               - Download TUH subset (20-30 GB)\n               - Use pre-extracted features for prototyping\n               - Incremental learning: train on batches\n               - Data streaming instead of loading all into memory\n\n            5. ANNOTATION QUALITY:\n               Problem: Seizure boundaries are subjective, inter-rater variability\n               Solutions:\n               - Use consensus annotations from TUH (multi-expert)\n               - Soft labels with uncertainty\n               - Attention to annotation guidelines\n               - Validate on multiple datasets\n               - Clinical expert review (Dr. Koubeissi)\n\n            6. ARTIFACT CONTAMINATION:\n               Problem: Eye blinks, muscle artifacts, electrode noise\n               Solutions:\n               - Rigorous preprocessing (ICA, wavelet denoising)\n               - Artifact detection and rejection\n               - Robust features less sensitive to artifacts\n               - Data augmentation with artificial artifacts\n               - Annotate artifact-heavy regions and exclude\n\n            7. OVERFITTING (Limited seizure events):\n               Problem: Models memorize training seizures\n               Solutions:\n               - Strong regularization (dropout 0.3-0.5)\n               - Data augmentation\n               - Early stopping based on validation set\n               - Cross-validation\n               - Simplify model architecture if overfitting persists\n               - Use pre-trained models (transfer learning)\n\n            8. REAL-TIME LATENCY:\n               Problem: Clinical deployment requires <1 second latency\n               Solutions:\n               - Model compression (pruning, quantization)\n               - Knowledge distillation (large model \u2192 small model)\n               - Efficient architectures (MobileNet-style)\n               - ONNX runtime for optimized inference\n               - Sliding window approach (process incrementally)\n               - GPU inference for speed\n\n            9. FALSE POSITIVES:\n               Problem: High false positive rate frustrates clinicians\n               Solutions:\n               - Post-processing: require sustained prediction (3+ consecutive windows)\n               - Ensemble methods to increase confidence\n               - Uncertainty thresholding (only alert if certain)\n               - Multi-stage pipeline: fast screener \u2192 accurate verifier\n               - Learn from false positives (add to training data)\n\n            10. REPRODUCIBILITY:\n                Problem: Random seeds, library versions, hardware differences\n                Solutions:\n                - Fix random seeds (torch.manual_seed, np.random.seed)\n                - Document exact library versions (requirements.txt)\n                - Use Docker containers for consistent environment\n                - Provide pre-trained models\n                - Detailed documentation of hyperparameters\n                - Share preprocessed data\n\n            11. CLINICAL VALIDATION:\n                Problem: Research metrics \u2260 clinical utility\n                Solutions:\n                - Collaborate with Dr. Koubeissi for clinical evaluation\n                - Use clinically relevant metrics (FPR/h, detection latency)\n                - Retrospective validation on held-out patients\n                - Prospective validation on new recordings\n                - User studies with clinicians (web demo)\n                - Compare against clinical EEG technicians\n\n            12. ETHICAL AND PRIVACY CONCERNS:\n                Problem: Patient data is sensitive\n                Solutions:\n                - Use only de-identified, publicly available datasets\n                - HIPAA compliance if deploying clinically\n                - Data encryption for storage and transmission\n                - Secure web application (HTTPS, authentication)\n                - No patient identifiers in publications\n                - IRB approval if collecting new data\n\n            RISK MITIGATION TIMELINE:\n            - Weeks 1-2: Validate data pipeline, ensure datasets are accessible\n            - Weeks 3-4: Verify baselines train correctly, establish infrastructure\n            - Weeks 5-6: Monitor for overfitting, implement regularization\n            - Weeks 7-8: Test transformer memory usage, optimize if needed\n            - Weeks 9-10: Validate GNN graph construction, test multi-branch fusion\n            - Weeks 11-12: Clinical validation, gather feedback from Dr. Koubeissi\n            - Weeks 13-14: Deployment testing, latency benchmarking, final validation\n            ",
  "Additional Resources": "\n            DATASET DOWNLOAD LINKS:\n\n            1. CHB-MIT (PRIMARY): \n               - PhysioNet: https://physionet.org/content/chbmit/1.0.0/\n               - Kaggle: https://www.kaggle.com/datasets/adibadea/chbmitseizuredataset\n               - Command: wget -r -N -c -np https://physionet.org/files/chbmit/1.0.0/\n\n            2. TUH Seizure Corpus:\n               - Official: https://isip.piconepress.com/projects/tuh_eeg/html/downloads.shtml\n               - Registration: Free, instant approval\n               - Download: Via provided scripts after registration\n\n            3. Siena Scalp EEG:\n               - PhysioNet: https://physionet.org/content/siena-scalp-eeg/1.0.0/\n               - Command: wget -r -N -c -np https://physionet.org/files/siena-scalp-eeg/1.0.0/\n\n            4. Bonn EEG:\n               - Official: http://epileptologie-bonn.de/cms/front_content.php?idcat=193\n               - Kaggle: https://www.kaggle.com/datasets/harunshimanto/epileptic-seizure-recognition\n\n\n            KEY LIBRARIES AND TOOLS:\n\n            EEG Processing:\n            - MNE-Python: https://mne.tools/ (standard for EEG/MEG analysis)\n            - PyEDFlib: https://github.com/holgern/pyedflib (EDF file I/O)\n            - YASA: https://github.com/raphaelvallat/yasa (sleep analysis, useful for features)\n\n            Deep Learning:\n            - PyTorch: https://pytorch.org/\n            - PyTorch Lightning: https://www.pytorchlightning.ai/ (training framework)\n            - Hugging Face Transformers: https://huggingface.co/transformers/\n            - TimesNet: https://github.com/thuml/TimesNet\n            - PyTorch Geometric: https://pytorch-geometric.readthedocs.io/ (for GNNs)\n\n            Signal Processing:\n            - SciPy: https://scipy.org/\n            - PyWavelets: https://pywavelets.readthedocs.io/\n            - Librosa: https://librosa.org/ (audio processing, applicable to time-series)\n\n            Visualization:\n            - Matplotlib: https://matplotlib.org/\n            - Seaborn: https://seaborn.pydata.org/\n            - Plotly: https://plotly.com/ (interactive plots)\n            - Weights & Biases: https://wandb.ai/ (experiment tracking)\n\n            Explainability:\n            - SHAP: https://github.com/slundberg/shap\n            - Captum: https://captum.ai/ (PyTorch interpretability)\n\n            Deployment:\n            - Streamlit: https://streamlit.io/\n            - Gradio: https://gradio.app/\n            - FastAPI: https://fastapi.tiangolo.com/ (REST API)\n            - ONNX: https://onnx.ai/ (model optimization)\n\n\n            PRE-TRAINED MODELS (For Transfer Learning):\n\n            General Deep Learning:\n            - ImageNet pre-trained CNNs (ResNet, EfficientNet) - adapt to 1D\n            - BERT, GPT models (for transformer initialization)\n\n            EEG-Specific (if available):\n            - Check Hugging Face Model Hub for EEG models\n            - PhysioNet published models\n            - GitHub repositories with pre-trained weights\n\n\n            PAPERS TO READ (ESSENTIAL):\n\n            Foundational:\n            1. \"A Large-Scale EEG Database for Epileptic Seizure Detection\" (CHB-MIT paper)\n            2. \"Attention Is All You Need\" (Transformer architecture)\n            3. \"BERT: Pre-training of Deep Bidirectional Transformers\" (BERT paper)\n\n            EEG & Seizure Detection:\n            4. \"Deep Learning for Electroencephalogram (EEG) Classification Tasks\"\n            5. \"Graph Convolutional Networks for EEG-based Emotion Recognition\"\n            6. \"A Survey on Deep Learning Methods for EEG Signals\"\n            7. \"Patient-Specific Seizure Prediction with Deep Learning\"\n\n            Transformers for Time-Series:\n            8. \"TimesNet: Temporal 2D-Variation Modeling for General Time Series Analysis\"\n            9. \"Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting\"\n            10. \"A Transformer-based Framework for Multivariate Time Series Representation Learning\"\n\n            Medical AI:\n            11. \"Clinically Applicable Deep Learning for Diagnosis from Medical Images\"\n            12. \"The Medical AI Safety Problem: A Unified Framework for Responsible Development\"\n\n\n            TUTORIALS AND COURSES:\n\n            EEG Analysis:\n            - MNE-Python Tutorials: https://mne.tools/stable/auto_tutorials/index.html\n            - Mike X Cohen's EEG Course: https://www.udemy.com/course/solved-challenges-ineeg/\n\n            Deep Learning:\n            - Fast.ai: https://www.fast.ai/\n            - Stanford CS230 (Deep Learning): https://cs230.stanford.edu/\n            - PyTorch Tutorials: https://pytorch.org/tutorials/\n\n            Medical AI:\n            - Stanford AI in Healthcare: https://stanfordmlgroup.github.io/\n\n\n            COMPUTING RESOURCES:\n\n            Free Options:\n            - Google Colab (free tier): 12-16 GB GPU RAM\n            - Google Colab Pro ($10/month): Better GPUs, longer runtime\n            - Kaggle Kernels: Free GPU (30h/week)\n            - GWU SEAS GPU Cluster: Apply for access\n\n            Paid Options:\n            - AWS EC2 (p3.2xlarge): ~$3/hour\n            - Google Cloud Platform: Similar pricing\n            - Lambda Labs: GPU cloud, competitive pricing\n\n\n            CLINICAL COLLABORATION:\n\n            - Dr. Mohamad Koubeissi (mkoubeissi@email.gwu.edu)\n              * Clinical neurologist, epilepsy specialist\n              * Provides clinical guidance and validation\n              * Can help with:\n                - Understanding EEG physiology\n                - Seizure type classification\n                - Clinical relevance of features\n                - Evaluation metrics from clinical perspective\n                - Potential deployment at GWU Hospital\n\n\n            CODE REPOSITORIES (For Reference):\n\n            1. EEG-Transformer: https://github.com/vlawhern/arl-eegmodels\n            2. Seizure Detection Examples: Search GitHub for \"seizure detection deep learning\"\n            3. MNE-Python Examples: https://github.com/mne-tools/mne-python/tree/main/examples\n\n\n            RECOMMENDED SOFTWARE SETUP:\n\n            ```bash\n            # Create conda environment\n            conda create -n eeg-seizure python=3.9\n            conda activate eeg-seizure\n\n            # Install PyTorch (with CUDA if GPU available)\n            conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia\n\n            # Install EEG libraries\n            pip install mne pyedflib\n\n            # Install ML libraries\n            pip install scikit-learn xgboost lightgbm\n\n            # Install deep learning utilities\n            pip install pytorch-lightning transformers timm einops\n\n            # Install signal processing\n            pip install scipy pywavelets librosa\n\n            # Install visualization\n            pip install matplotlib seaborn plotly wandb\n\n            # Install explainability\n            pip install shap captum\n\n            # Install deployment\n            pip install streamlit gradio fastapi uvicorn\n\n            # Install GNN libraries\n            pip install torch-geometric torch-scatter torch-sparse\n\n            # Install Jupyter for notebooks\n            pip install jupyter ipywidgets\n            ```\n\n\n            DOCUMENTATION CHECKLIST:\n\n            README.md should include:\n            - Project overview and motivation\n            - Installation instructions\n            - Dataset download guide\n            - Preprocessing pipeline explanation\n            - Model training commands\n            - Evaluation scripts\n            - Pre-trained model links\n            - Web app deployment guide\n            - Citation information\n            - License (MIT or Apache 2.0)\n            - Acknowledgments\n\n            Code Documentation:\n            - Docstrings for all functions and classes\n            - Type hints (Python 3.9+)\n            - Inline comments for complex logic\n            - Configuration files (YAML or JSON)\n            - Example usage in notebooks\n\n\n            PROFESSIONAL DEVELOPMENT:\n\n            Conferences to Attend (Virtual Options Available):\n            - MIDL 2026 (July)\n            - MICCAI 2026 (October)\n            - NeurIPS 2026 (December)\n            - American Epilepsy Society Meeting\n\n            Networking:\n            - Join EEG/Epilepsy research communities on Twitter/X\n            - Participate in Kaggle competitions (seizure prediction)\n            - Contribute to open-source EEG libraries (MNE-Python)\n            - Present at GWU research seminars\n            ",
  "Proposed by": "Dr. Mohamad Koubeissi ",
  "Proposed by email": "mkoubeissi@email.gwu.edu",
  "instructor": "Amir Jafari",
  "instructor_email": "ajafari@gwu.edu",
  "collaborator": "Dr. Mohamad Koubeissi (Clinical Neurologist, GWU Medical Faculty)",
  "funding_opportunity": "NIH R01 for Epilepsy Research / NSF CAREER / GWU Cross-Disciplinary Research Fund",
  "github_repo": "https://github.com/amir-jafari"
}