{
  "Version": "1",
  "Year": "2026",
  "Semester": "Spring",
  "project_name": "Multimodal Financial Market Prediction: Integrating Sentiment Analysis and Time Series Forecasting",
  "Objective": "\n            The goal of this project is to develop a multimodal deep learning framework that combines financial time \n            series data with natural language sentiment from news articles and social media to predict stock market \n            movements. Students will explore how textual sentiment influences market dynamics and build interpretable \n            models that can explain predictions through attention mechanisms.\n\n            Key Objectives:\n            1. Build a comprehensive data pipeline that merges historical stock prices, financial news, and social \n               media sentiment into a unified time-aligned dataset.\n            2. Develop transformer-based architectures that can jointly model temporal dependencies in price movements \n               and semantic information from text.\n            3. Implement attention visualization techniques to identify which news events or sentiment shifts most \n               strongly correlate with price movements.\n            4. Compare performance against baseline models (ARIMA, LSTM-only, BERT-only) to demonstrate the value \n               of multimodal fusion.\n            5. Conduct ablation studies to understand the contribution of different data modalities and architectural \n               components.\n            6. Package the framework as an open-source tool with clear documentation for reproducibility.\n            ",
  "Dataset": "\n            All datasets are publicly available with no access restrictions:\n\n            PRIMARY DATASETS (Immediate Download):\n\n            1. Stock Price Data:\n               - Yahoo Finance Historical Data (yfinance Python library - no API key needed)\n               - Tickers: S&P 500 stocks (SPY, AAPL, MSFT, GOOGL, AMZN, TSLA, etc.)\n               - Features: Open, High, Low, Close, Volume, Adjusted Close\n               - Time Range: 2015-2024 (adjustable)\n               - Download: pip install yfinance; import yfinance as yf; yf.download('AAPL', start='2015-01-01')\n\n            2. Financial News Sentiment:\n               - Kaggle: \"Financial News and Stock Price Integration Dataset (FNSPID)\"\n               - URL: https://www.kaggle.com/datasets/miguelaenlle/massive-stock-news-analysis-db-for-nlpbacktests\n               - Size: 6+ million news articles mapped to stock tickers\n               - Direct Download: Yes (Kaggle API or manual download)\n\n            3. Twitter Financial Sentiment:\n               - Kaggle: \"Twitter Financial News Sentiment\"\n               - URL: https://www.kaggle.com/datasets/sulphatet/twitter-financial-news\n               - Size: 11,932 tweets with sentiment labels\n               - Direct Download: Yes\n\n            4. Alternative: StockNet Dataset\n               - URL: https://github.com/yumoxu/stocknet-dataset\n               - Contains: Historical prices + tweets for 88 stocks\n               - Direct Download: GitHub clone\n\n            SUPPLEMENTARY DATASETS (Optional Enhancement):\n\n            5. Financial PhraseBank (for sentiment model fine-tuning):\n               - URL: https://www.researchgate.net/publication/251231364_FinancialPhraseBank-v10\n               - Size: 4,840 sentences with sentiment annotations\n               - Direct Download: Yes\n\n            6. Reddit WallStreetBets Data:\n               - Kaggle: \"Reddit WallStreetBets Posts\"\n               - URL: https://www.kaggle.com/datasets/gpreda/reddit-wallstreetsbets-posts\n               - Direct Download: Yes\n\n            DATASET PREPARATION:\n            Students will align textual data with stock prices by timestamp, creating sequences where each time step \n            contains: [price_features, technical_indicators, aggregated_sentiment, news_embeddings].\n            ",
  "Rationale": "\n            Financial markets are influenced by both quantitative factors (historical prices, trading volumes) and \n            qualitative factors (news sentiment, investor emotions, market narratives). Traditional time series models \n            often ignore textual information, while pure NLP approaches fail to capture temporal market dynamics.\n\n            WHY THIS PROJECT IS TIMELY AND PUBLISHABLE:\n\n            1. EMERGING RESEARCH AREA: Multimodal fusion for finance is a hot topic in top-tier venues (NeurIPS, ICML, \n               AAAI, KDD). Recent papers show 10-15% improvement over unimodal baselines.\n\n            2. TRANSFORMER REVOLUTION: Financial time series transformers (Temporal Fusion Transformer, Informer, \n               Autoformer) are state-of-the-art, but few integrate them with language models effectively.\n\n            3. INTERPRETABILITY DEMAND: Regulators and practitioners need explainable AI for financial decisions. \n               Attention-based explanations showing \"which news drove this prediction\" are highly valued.\n\n            4. PRACTICAL IMPACT: Hedge funds, trading firms, and fintech companies actively seek such models. \n               Publishing code increases adoption and citations.\n\n            5. NOVELTY OPPORTUNITIES: Students can contribute by:\n               - Testing different fusion strategies (early vs. late fusion, cross-attention)\n               - Exploring causal inference (does sentiment predict returns or vice versa?)\n               - Handling market regime changes (bull vs. bear markets)\n               - Multi-stock correlation modeling with graph neural networks\n\n            PUBLICATION VENUES:\n            - ML Conferences: ICML, NeurIPS, ICLR (workshop tracks)\n            - AI + Finance: AAAI Workshop on AI in Finance, KDD FinTech Day\n            - Finance Journals: Journal of Finance and Data Science, Expert Systems with Applications\n            - Workshops: ACL Workshop on Economics and NLP, EMNLP FinNLP Workshop\n            ",
  "Approach": "\n            PHASE 1: DATA COLLECTION & PREPROCESSING (Weeks 1-2)\n\n            [Week 1: Data Acquisition]\n            - Set up yfinance to download historical prices for 20-50 stocks (diverse sectors)\n            - Download financial news dataset from Kaggle\n            - Download Twitter sentiment dataset from Kaggle\n            - Create unified directory structure for raw data\n            - Document data statistics: timeframes, missing values, label distributions\n\n            [Week 2: Data Preprocessing]\n            - Clean and normalize stock price data (handle splits, dividends)\n            - Calculate technical indicators: RSI, MACD, Bollinger Bands, Moving Averages\n            - Preprocess text: remove URLs, mentions, hashtags; lowercase; tokenization\n            - Timestamp alignment: map each news article/tweet to corresponding trading day\n            - Aggregate multiple news items per day using: max sentiment, mean sentiment, count\n            - Create train/validation/test splits: 70%/15%/15% with temporal ordering preserved\n            - Handle class imbalance (up/down/neutral movements) via stratification or weighting\n\n\n            PHASE 2: BASELINE MODELS (Weeks 3-4)\n\n            [Week 3: Time Series Baselines]\n            Implement and evaluate classical and deep learning time series models:\n            - ARIMA: Classical statistical baseline\n            - LSTM: Univariate (price only) recurrent model\n            - GRU: Lighter alternative to LSTM\n            - Temporal Convolutional Network (TCN): Dilated convolutions for time series\n\n            Metrics: Accuracy, Precision, Recall, F1-Score, ROC-AUC, Sharpe Ratio (if applicable)\n\n            [Week 4: NLP Baselines]\n            Implement sentiment-only classification models:\n            - TF-IDF + Logistic Regression: Classical NLP baseline\n            - FinBERT: Pre-trained BERT for financial sentiment (Hugging Face: ProsusAI/finbert)\n            - RoBERTa: Fine-tuned on financial news for sentiment classification\n\n            Aggregate daily sentiment and predict next-day returns using only textual features.\n\n\n            PHASE 3: MULTIMODAL FUSION ARCHITECTURES (Weeks 5-8)\n\n            [Week 5-6: Early Fusion Architecture]\n            Concatenate time series features with text embeddings at input level:\n\n            Architecture:\n            1. Text Encoder: FinBERT \u2192 [CLS] token embedding (768-dim)\n            2. Price Encoder: 1D-CNN or LSTM \u2192 temporal feature vector (128-dim)\n            3. Concatenate: [price_features, text_embedding] \u2192 896-dim vector\n            4. Feed to MLP classifier: Dense(512) \u2192 ReLU \u2192 Dropout(0.3) \u2192 Dense(3) for {up, down, neutral}\n\n            Training:\n            - Loss: Cross-Entropy with class weights\n            - Optimizer: AdamW with learning rate 1e-4\n            - Batch size: 32\n            - Epochs: 50 with early stopping (patience=5)\n\n            [Week 7-8: Late Fusion with Cross-Attention]\n            Process modalities separately, then fuse using attention:\n\n            Architecture:\n            1. Text Branch: FinBERT \u2192 sequence of token embeddings\n            2. Price Branch: Transformer Encoder (3 layers) \u2192 sequence of temporal embeddings\n            3. Cross-Attention: Query=price embeddings, Key/Value=text embeddings\n               - Allows model to attend to relevant news when predicting price movements\n            4. Fusion: Concatenate attended representations\n            5. Classifier: Transformer Encoder \u2192 MLP head\n\n            Implementation:\n            - Use PyTorch nn.MultiheadAttention\n            - Visualize attention weights to identify important news phrases\n\n            [Advanced Option: Temporal Fusion Transformer]\n            Adapt Google's TFT architecture to incorporate text:\n            - Variable selection networks for feature importance\n            - Multi-horizon forecasting (predict 1-day, 5-day, 20-day returns)\n\n\n            PHASE 4: INTERPRETABILITY & ANALYSIS (Weeks 9-10)\n\n            [Week 9: Attention Visualization]\n            - Extract attention weights from cross-attention layer\n            - Identify which news headlines/sentences receive highest attention before major price movements\n            - Create heatmaps showing word-level importance\n            - Case studies: Analyze specific events (earnings reports, Fed announcements, geopolitical events)\n\n            [Week 10: Ablation Studies]\n            Systematically remove components to measure contribution:\n            - Ablation 1: Remove all text features (time series only)\n            - Ablation 2: Remove all price features (sentiment only)\n            - Ablation 3: Remove attention mechanism (simple concatenation)\n            - Ablation 4: Remove technical indicators (raw prices only)\n\n            Metrics: Track performance degradation to quantify each component's value\n\n\n            PHASE 5: ADVANCED EXPERIMENTS (Weeks 11-12)\n\n            [Week 11: Multi-Stock Modeling]\n            - Extend to predict movements for multiple stocks simultaneously\n            - Option 1: Graph Neural Network (GNN) where stocks are nodes, correlations are edges\n            - Option 2: Shared encoder with stock-specific heads\n            - Analyze cross-stock information flow\n\n            [Week 12: Market Regime Analysis]\n            - Segment data into bull markets (sustained uptrend) vs. bear markets (downtrend)\n            - Train separate models or use regime as additional feature\n            - Evaluate if sentiment has different predictive power in different market conditions\n            - Test on 2020 COVID crash, 2022 inflation crisis for robustness\n\n\n            PHASE 6: PAPER WRITING & CODE RELEASE (Weeks 13-14)\n\n            [Week 13: Research Paper]\n            Structure (following ACL/AAAI format):\n            1. Abstract: Problem, method, results (200 words)\n            2. Introduction: Motivation, research questions, contributions\n            3. Related Work: Time series forecasting, financial NLP, multimodal learning\n            4. Methodology: Architecture diagrams, fusion strategies, training details\n            5. Experiments: Datasets, baselines, evaluation metrics\n            6. Results: Tables comparing all models, ablation studies, attention visualizations\n            7. Discussion: When does sentiment help? Failure cases? Limitations?\n            8. Conclusion: Findings, future work\n\n            [Week 14: Code & Documentation]\n            - Create GitHub repository with MIT license\n            - README: Installation, dataset download scripts, training commands\n            - Jupyter notebooks: Data exploration, model training, result visualization\n            - Requirements.txt with all dependencies\n            - Pre-trained model weights (Hugging Face Hub)\n            - Documentation: Detailed API docs for all modules\n\n\n            CODE STRUCTURE:\n\n            ```\n            financial-sentiment-prediction/\n            \u2502\n            \u251c\u2500\u2500 data/\n            \u2502   \u251c\u2500\u2500 download_data.py          # Scripts to fetch datasets\n            \u2502   \u251c\u2500\u2500 preprocess.py              # Data cleaning and alignment\n            \u2502   \u2514\u2500\u2500 README.md                  # Data documentation\n            \u2502\n            \u251c\u2500\u2500 models/\n            \u2502   \u251c\u2500\u2500 baselines.py               # ARIMA, LSTM, FinBERT\n            \u2502   \u251c\u2500\u2500 early_fusion.py            # Concatenation-based fusion\n            \u2502   \u251c\u2500\u2500 late_fusion.py             # Cross-attention fusion\n            \u2502   \u2514\u2500\u2500 temporal_fusion.py         # TFT adaptation\n            \u2502\n            \u251c\u2500\u2500 training/\n            \u2502   \u251c\u2500\u2500 train.py                   # Training loop\n            \u2502   \u251c\u2500\u2500 evaluate.py                # Evaluation metrics\n            \u2502   \u2514\u2500\u2500 config.yaml                # Hyperparameters\n            \u2502\n            \u251c\u2500\u2500 analysis/\n            \u2502   \u251c\u2500\u2500 attention_viz.py           # Visualize attention weights\n            \u2502   \u251c\u2500\u2500 ablation.py                # Ablation experiments\n            \u2502   \u2514\u2500\u2500 case_studies.ipynb         # Event analysis\n            \u2502\n            \u251c\u2500\u2500 notebooks/\n            \u2502   \u251c\u2500\u2500 01_data_exploration.ipynb\n            \u2502   \u251c\u2500\u2500 02_baseline_results.ipynb\n            \u2502   \u2514\u2500\u2500 03_final_results.ipynb\n            \u2502\n            \u251c\u2500\u2500 requirements.txt\n            \u251c\u2500\u2500 README.md\n            \u2514\u2500\u2500 LICENSE\n            ```\n            ",
  "Timeline": "\n            Week 1-2:   Data Collection & Preprocessing\n            Week 3-4:   Baseline Models (Time Series + NLP separately)\n            Week 5-6:   Early Fusion Architecture\n            Week 7-8:   Late Fusion with Cross-Attention\n            Week 9-10:  Interpretability & Ablation Studies\n            Week 11-12: Advanced Experiments (Multi-stock, Regime Analysis)\n            Week 13:    Research Paper Writing\n            Week 14:    Code Release & Documentation\n\n            TOTAL: 14 weeks (one semester)\n\n            MILESTONES:\n            - Week 2: Data pipeline complete\n            - Week 4: Baseline results documented\n            - Week 8: Multimodal models trained\n            - Week 10: Ablation studies complete\n            - Week 14: Paper submitted to workshop, code on GitHub\n            ",
  "Expected Number Students": "\n            RECOMMENDED: 2-3 students\n\n            ROLE DISTRIBUTION (for 3 students):\n\n            Student 1: Data Engineer & Time Series Specialist\n            - Responsibilities: Data collection, preprocessing, temporal feature engineering\n            - Focus: Baseline time series models (ARIMA, LSTM, TCN)\n            - Deliverables: Clean datasets, time series baseline results\n\n            Student 2: NLP & Sentiment Analysis Specialist\n            - Responsibilities: Text preprocessing, sentiment model fine-tuning\n            - Focus: FinBERT training, sentiment baselines\n            - Deliverables: Text embeddings, NLP baseline results\n\n            Student 3: Deep Learning & Fusion Architect\n            - Responsibilities: Multimodal architecture design, attention mechanisms\n            - Focus: Early/late fusion models, interpretability\n            - Deliverables: Fusion architectures, attention visualizations\n\n            SHARED RESPONSIBILITIES:\n            - All students: Paper writing, code documentation, presentation\n            - Weekly meetings to integrate components and discuss results\n\n            FOR 2 STUDENTS:\n            - Student 1: Data + Time Series + Fusion\n            - Student 2: NLP + Sentiment + Interpretability\n            ",
  "Research Contributions": "\n            This project offers multiple avenues for novel research contributions:\n\n            1. METHODOLOGICAL CONTRIBUTIONS:\n            - Novel cross-attention mechanism specifically designed for price-news alignment\n            - Temporal aggregation strategies for daily sentiment from multiple news sources\n            - Multi-horizon prediction framework (predict 1-day, 5-day, 20-day movements jointly)\n            - Graph-based multi-stock modeling with sentiment propagation\n\n            2. EMPIRICAL CONTRIBUTIONS:\n            - Comprehensive benchmark comparing 10+ baseline models\n            - Ablation study quantifying value of each modality and architectural component\n            - Analysis of sentiment predictive power across different market regimes\n            - Case studies on major market events (earnings, Fed decisions, black swans)\n\n            3. INTERPRETABILITY CONTRIBUTIONS:\n            - Attention-based explanation showing which news influenced predictions\n            - Saliency maps highlighting important words/phrases in financial text\n            - Temporal importance analysis: which time lags matter most?\n\n            4. PRACTICAL CONTRIBUTIONS:\n            - Open-source framework enabling reproducible research\n            - Pre-trained models for financial sentiment analysis\n            - Documentation and tutorials for practitioners\n\n            PUBLICATION STRATEGY:\n\n            Target Venues (in order of preference):\n            1. Workshop Papers (easier acceptance for students):\n               - NeurIPS Workshop on Machine Learning in Finance\n               - AAAI Workshop on AI for Financial Services\n               - ACL Workshop on Economics and NLP (ECONLP)\n               - EMNLP FinNLP Workshop\n\n            2. Main Conference Papers (if results are strong):\n               - KDD (ACM SIGKDD Conference on Knowledge Discovery and Data Mining)\n               - AAAI (Association for the Advancement of Artificial Intelligence)\n               - IJCAI (International Joint Conference on Artificial Intelligence)\n\n            3. Journal Papers (after workshop feedback):\n               - Expert Systems with Applications\n               - Journal of Finance and Data Science\n               - Information Sciences\n\n            EXPECTED OUTCOMES:\n            - 1 workshop paper submission (Week 14)\n            - 1 GitHub repository with 50+ stars (within 6 months)\n            - 1 blog post on Medium/Towards Data Science\n            - Potential collaboration with finance industry partners\n\n            BROADER IMPACT:\n            - Democratize access to advanced trading signals\n            - Improve financial market efficiency through better information processing\n            - Provide educational resource for students learning multimodal ML\n            - Foster transparency in algorithmic trading through interpretability\n            ",
  "Possible Issues": "\n            TECHNICAL CHALLENGES:\n\n            1. Data Quality & Alignment:\n            - ISSUE: News timestamps may not align perfectly with trading hours\n            - SOLUTION: Aggregate news from previous day's close to current day's open\n\n            2. Class Imbalance:\n            - ISSUE: Market movements may be heavily skewed (e.g., 60% up, 30% down, 10% neutral)\n            - SOLUTION: Use class weights in loss function, stratified sampling, or SMOTE\n\n            3. Lookahead Bias:\n            - ISSUE: Accidentally using future information in predictions\n            - SOLUTION: Strict temporal validation split, careful feature engineering\n\n            4. Computational Resources:\n            - ISSUE: Training large transformers on long sequences is memory-intensive\n            - SOLUTION: Use gradient accumulation, mixed precision training (fp16), or smaller models\n\n            5. Overfitting to Noise:\n            - ISSUE: Financial markets have low signal-to-noise ratio\n            - SOLUTION: Strong regularization (dropout 0.3-0.5), early stopping, cross-validation\n\n            6. Sentiment Ambiguity:\n            - ISSUE: Financial news sentiment is often nuanced (e.g., \"earnings beat expectations but guidance lowered\")\n            - SOLUTION: Use aspect-based sentiment analysis, fine-tune FinBERT on domain data\n\n\n            PRACTICAL CHALLENGES:\n\n            7. Baseline Comparisons:\n            - ISSUE: Students may struggle to implement all baselines correctly\n            - SOLUTION: Use existing libraries (statsmodels for ARIMA, Hugging Face for FinBERT)\n\n            8. Evaluation Metrics:\n            - ISSUE: Accuracy alone is insufficient for financial applications\n            - SOLUTION: Report Precision/Recall/F1, ROC-AUC, and trading simulation metrics (Sharpe ratio)\n\n            9. Reproducibility:\n            - ISSUE: Random seeds, library versions can cause different results\n            - SOLUTION: Pin all library versions, set seeds explicitly, document hardware specs\n\n            10. Time Management:\n            - ISSUE: 14 weeks is tight for complex multimodal project\n            - SOLUTION: Use pre-trained models (FinBERT), focus on fusion over training from scratch\n\n\n            RESEARCH CHALLENGES:\n\n            11. Market Efficiency Paradox:\n            - ISSUE: If model works too well, market would arbitrage it away\n            - SOLUTION: Frame as \"information processing improvement\" not \"guaranteed returns\"\n\n            12. Publication Standards:\n            - ISSUE: Finance + ML papers require both technical rigor and domain knowledge\n            - SOLUTION: Consult with finance faculty, cite seminal papers, use correct terminology\n\n            13. Ethical Considerations:\n            - ISSUE: Models could be used for market manipulation or unfair advantage\n            - SOLUTION: Include ethics section in paper, release code responsibly, avoid HFT claims\n\n\n            MITIGATION STRATEGIES:\n\n            - Weekly check-ins with advisor to catch issues early\n            - Incremental development: get baselines working before attempting complex architectures\n            - Code reviews between team members to catch bugs\n            - Maintain detailed experiment logs (WandB, MLflow)\n            - Have backup publication venue if main target rejects\n            - Budget extra time for debugging (expect 20% of timeline for troubleshooting)\n            ",
  "Additional Resources": "\n            DATASET DOWNLOAD LINKS (All Immediately Accessible):\n\n            1. Stock Price Data:\n               - Library: pip install yfinance\n               - Documentation: https://pypi.org/project/yfinance/\n               - Example Code:\n                 import yfinance as yf\n                 data = yf.download('AAPL', start='2015-01-01', end='2024-12-31')\n\n            2. Financial News Dataset:\n               - Kaggle: https://www.kaggle.com/datasets/miguelaenlle/massive-stock-news-analysis-db-for-nlpbacktests\n               - Download: Use Kaggle API or manual download (requires free Kaggle account)\n               - kaggle datasets download -d miguelaenlle/massive-stock-news-analysis-db-for-nlpbacktests\n\n            3. Twitter Financial Sentiment:\n               - Kaggle: https://www.kaggle.com/datasets/sulphatet/twitter-financial-news\n               - Download: kaggle datasets download -d sulphatet/twitter-financial-news\n\n            4. StockNet (Alternative):\n               - GitHub: https://github.com/yumoxu/stocknet-dataset\n               - Download: git clone https://github.com/yumoxu/stocknet-dataset.git\n\n            5. FinancialPhraseBank:\n               - Direct: https://www.researchgate.net/publication/251231364_FinancialPhraseBank-v10\n               - Also on Hugging Face: datasets.load_dataset(\"financial_phrasebank\", \"sentences_allagree\")\n\n            6. Reddit WallStreetBets:\n               - Kaggle: https://www.kaggle.com/datasets/gpreda/reddit-wallstreetsbets-posts\n               - Download: kaggle datasets download -d gpreda/reddit-wallstreetsbets-posts\n\n\n            PRE-TRAINED MODELS (Hugging Face):\n\n            1. FinBERT (Financial Sentiment):\n               - Model: ProsusAI/finbert\n               - Usage: transformers.AutoModelForSequenceClassification.from_pretrained('ProsusAI/finbert')\n\n            2. DistilBERT-Financial-Sentiment:\n               - Model: mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis\n\n            3. RoBERTa-Financial:\n               - Model: cardiffnlp/twitter-roberta-base-sentiment\n\n\n            REFERENCE PAPERS:\n\n            Key Papers to Cite:\n            1. \"StockNet: Deep Learning for Stock Movement Prediction\" (Xu & Cohen, ACL 2018)\n            2. \"Listening to Chaotic Whispers: A Deep Learning Framework for News-oriented Stock Trend Prediction\" (Hu et al., WSDM 2018)\n            3. \"FinBERT: Financial Sentiment Analysis with Pre-trained Language Models\" (Araci, 2019)\n            4. \"Temporal Fusion Transformers for Interpretable Multi-horizon Time Series Forecasting\" (Lim et al., 2021)\n            5. \"Attention Is All You Need\" (Vaswani et al., NeurIPS 2017)\n\n\n            TUTORIALS & CODE REPOSITORIES:\n\n            1. Hugging Face Financial NLP:\n               - https://huggingface.co/blog/sentiment-analysis-python\n\n            2. Time Series with Transformers:\n               - https://huggingface.co/blog/time-series-transformers\n\n            3. PyTorch Multimodal Tutorial:\n               - https://pytorch.org/tutorials/beginner/multimodal_tutorial.html\n\n            4. Example GitHub Repos:\n               - https://github.com/microsoft/qlib (Quantitative Finance Library)\n               - https://github.com/huseinzol05/Stock-Prediction-Models\n\n\n            TOOLS & LIBRARIES:\n\n            Required Libraries (requirements.txt):\n            - torch>=2.0.0\n            - transformers>=4.30.0\n            - yfinance>=0.2.0\n            - pandas>=2.0.0\n            - numpy>=1.24.0\n            - scikit-learn>=1.3.0\n            - matplotlib>=3.7.0\n            - seaborn>=0.12.0\n            - tqdm>=4.65.0\n            - wandb>=0.15.0 (for experiment tracking)\n            - ta (technical analysis library)\n            - statsmodels (for ARIMA)\n\n            Development Tools:\n            - Jupyter Notebook / JupyterLab\n            - VS Code with Python extension\n            - Git / GitHub for version control\n            - Weights & Biases or MLflow for experiment tracking\n            ",
  "Proposed by": "Dr. Amir Jafari",
  "Proposed by email": "ajafari@gwu.edu",
  "instructor": "Amir Jafari",
  "instructor_email": "ajafari@gwu.edu",
  "collaborator": "None",
  "github_repo": "https://github.com/amir-jafari"
}