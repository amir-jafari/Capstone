<h1>Capstone Proposal</h1>
<h2>modrl</h2>
<h3>Proposed by: Tyler Wallett</h3>
<h4>Email: twallett@gwu.edu</h4>
<h4>Advisor: Amir Jafari</h4>
<h4>The George Washington University, Washington DC</h4>
<h4>Data Science Program</h4>
<h2>1 Objective:</h2>
<pre><code>        Establish version == 1.0.0 of a modular Bandit, Classical and Deep RL Python Library with notation, examples and applications.
</code></pre>
<p><img src="2026_Spring_3.png" alt="Figure 1: Example figure">
<em>Figure 1: Caption</em></p>
<h2>2 Dataset:</h2>
<pre><code>        - Bandits: Synthetic NumPy probability distribution or datasets contingent on application.
        - Classical and Deep RL: Integration with Gymnasium or Farama Foundation Environments (env.reset(), env.step(), env.close())
</code></pre>
<h2>3 Rationale:</h2>
<pre><code>        Most RL Python Libraries are not modular or flexible. These libraries run in standard .train() and .predict()
        functionality which restricts researchers and practicioners.

        This project aims to develop a library that enables the user to have more of a modular functioanlity when it comes to 
        training Bandit and RL frameworks.
</code></pre>
<h2>4 Approach:</h2>
<pre><code>        In short, the capstone project will consists of three phases:
              1. Package functionality and examples development (weeks 1-9)
              2. Sphinx front-end documentation (weeks 10-11)
              3. Research paper preparation (weeks 12-14)
</code></pre>
<h2>5 Timeline:</h2>
<pre><code>        Week 1:     `modrl/modrl/bandits/classical` (classical bandit algorithms: eg, ucb, ts)
        Week 2:     `modrl/modrl/evaluation` (classical bandit performance evaluation)
        Week 3:     `modrl/examples/bandits/classical` (collab notebook examples of classical bandit algorithms)
        Week 4:     `modrl/modrl/agents/classical` (classical RL: mc (on-off first visit), td (sarsa, q, doubleq))
        Week 5:     `modrl/modrl/evaluation` (classical RL performance evaluation)
        Week 6:     `modrl/examples/agents/classical` (collab notebook examples of classical RL algorithms)
        Week 7:     `modrl/modrl/agents/deep` (deep RL: dqn, vpg, ppo)
        Week 8:     `modrl/modrl/evaluation` (deep RL performance evaluation)
        Week 9:     (presentation)`modrl/examples/agents/deep` (collab notebook examples of deep RL algorithms)
        Week 10:    `sphinx` front-end docs: structure, notation, args, and returns
        Week 11:    `sphinx` front-end docs: style, collab examples, installation, and cheatsheets
        Week 12:    (package-live and research paper) research paper structure
        Week 13:    (research paper) draft research paper revision
        Week 14:    (research paper) final research paper revision

        Notes: since algorithm code will be provided for weeks 1, 4, and 7; students may create additional algorithms or 
        applications, if needed.
</code></pre>
<h2>6 Expected Number Students:</h2>
<pre><code>        One or maximum two students that have previously taken `DATS 6450: Reinforcement Learning` and attained a grade of A.
</code></pre>
<h2>7 Possible Issues:</h2>
<pre><code>        Collaboration and time-scheduling conflicts. 
</code></pre>
<h2>Contact</h2>
<ul>
<li>Author: Amir Jafari</li>
<li>Email: <a href="mailto:ajafari@gwu.edu">ajafari@gwu.edu</a></li>
<li>GitHub: <a href="https://github.com/https://github.com/twallett/modrl">https://github.com/twallett/modrl</a></li>
</ul>
