{
  "Version": "3",
  "Year": "2026",
  "Semester": "Spring",
  "project_name": "modrl",
  "Objective": "\n            Establish version == 1.0.0 of a modular Bandit, Classical and Deep RL Python Library with notation, examples and applications.\n            ",
  "Dataset": "\n            - Bandits: Synthetic NumPy probability distribution or datasets contingent on application.\n            - Classical and Deep RL: Integration with Gymnasium or Farama Foundation Environments (env.reset(), env.step(), env.close())\n            ",
  "Rationale": "\n            Most RL Python Libraries are not modular or flexible. These libraries run in standard .train() and .predict()\n            functionality which restricts researchers and practicioners.\n\n            This project aims to develop a library that enables the user to have more of a modular functioanlity when it comes to \n            training Bandit and RL frameworks.\n            ",
  "Approach": "\n            In short, the capstone project will consists of three phases:\n                  1. Package functionality and examples development (weeks 1-9)\n                  2. Sphinx front-end documentation (weeks 10-11)\n                  3. Research paper preparation (weeks 12-14)\n            ",
  "Timeline": "\n            Week 1:     `modrl/modrl/bandits/classical` (classical bandit algorithms: eg, ucb, ts)\n            Week 2:     `modrl/modrl/evaluation` (classical bandit performance evaluation)\n            Week 3:     `modrl/examples/bandits/classical` (collab notebook examples of classical bandit algorithms)\n            Week 4:     `modrl/modrl/agents/classical` (classical RL: mc (on-off first visit), td (sarsa, q, doubleq))\n            Week 5:     `modrl/modrl/evaluation` (classical RL performance evaluation)\n            Week 6:     `modrl/examples/agents/classical` (collab notebook examples of classical RL algorithms)\n            Week 7:     `modrl/modrl/agents/deep` (deep RL: dqn, vpg, ppo)\n            Week 8:     `modrl/modrl/evaluation` (deep RL performance evaluation)\n            Week 9:     (presentation)`modrl/examples/agents/deep` (collab notebook examples of deep RL algorithms)\n            Week 10:    `sphinx` front-end docs: structure, notation, args, and returns\n            Week 11:    `sphinx` front-end docs: style, collab examples, installation, and cheatsheets\n            Week 12:    (package-live and research paper) research paper structure\n            Week 13:    (research paper) draft research paper revision\n            Week 14:    (research paper) final research paper revision\n\n            Notes: since algorithm code will be provided for weeks 1, 4, and 7; students may create additional algorithms or \n            applications, if needed.\n            ",
  "Expected Number Students": "\n            One or maximum two students that have previously taken `DATS 6450: Reinforcement Learning` and attained a grade of A.\n            ",
  "Research Contributions": "\n            - Modular RL functionality.\n            - Seamless gymnasium Integration.\n            - Educational Alignment.\n            - Designed for Research and Experimentation.\n            - Open Source and Community-Driven.\n            ",
  "Possible Issues": "\n            Collaboration and time-scheduling conflicts. \n            ",
  "Additional Resources": "\n            Code and meeting time with Tyler Wallett will be provided.\n            ",
  "Proposed by": "Tyler Wallett",
  "Proposed by email": "twallett@gwu.edu",
  "instructor": "Amir Jafari",
  "instructor_email": "ajafari@gwu.edu",
  "collaborator": "None",
  "funding_opportunity": "I wish :(",
  "github_repo": "https://github.com/twallett/modrl"
}